<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[实验室级别深度学习装机教程]]></title>
    <url>%2F2017%2F11%2F30%2F%E5%AE%9E%E9%AA%8C%E5%AE%A4%E7%BA%A7%E5%88%AB%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%A3%85%E6%9C%BA%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[服务器配置主板：华硕X99-aII CPU: Intel i7-6850K 显卡：华硕GTX 1080Ti OC * 2 内存：海盗船复仇者8G*4 2400MHZ SSD: 三星SM961 256G 电源：海盗船AX1200i 全模组 机箱：海盗船AIR 540 散热器：海盗船H100i 一体式双排水冷 一、系统安装采用Ubuntu16.04，具体安装过程参见我之前的博客 本次安装也出现了较双系统安装从未出现的问题： U盘启动盘不能被正常引导，开机后直接进入黑屏。。可以通过临时修改grub进入安装界面 开机启动后按住Esc或是Right Shift进入以下界面 选择第一项，按下e键就进入以下 将图中的quiet splash 改为 nomodeset，然后按Ctrl+X进行boot，即可进入安装界面。 以上方法仅仅是解决了第一次安装系统时不能正常引导的问题，当成功安装系统后重启，就会再次出现黑屏界面，所以要想从根本上解决问题，就必须更新grub。 $ sudo nano /etc/default/grub 添加nomodeset到GRUB_CMDLINE_LINUX_DEFAULT: GRUB_DEFAULT=0 GRUB_HIDDEN_TIMEOUT=0 GRUB_HIDDEN_TIMEOUT_QUIET=true GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=lsb_release -i -s 2&gt; /dev/null || echo Debian GRUB_CMDLINE_LINUX_DEFAULT=”quiet splash nomodeset” GRUB_CMDLINE_LINUX=”” 然后Ctrl+O保存，Ctrl+X退出 $ sudo update-grub 这样就不会再出现开机黑屏的现象。 二、安装CUDA、CUDNN 英伟达GPU加速框架1、安装显卡驱动。 先看一下自己的电脑对应的驱动版本。 使用命令：ubuntu-drivers devices 会显示一个recommend的驱动版本。一般是三位数的，比如1080的显卡是384，950对应的是375。 得到版本号之后，输入命令： $ sudo apt-get install nvidia-384 //最后三位是自己显卡驱动的版本号 2、安装CUDA8.0点击下载 进去之后，选择Linux-&gt;x86_64-&gt;Ubuntu-&gt;16.04-&gt;runfile(local) 即可下载，大概是一个多G，如果需要注册啥的，注册个就好。 注意：以上只是一种下载CUDA8.0的方式。 当然你可以通过命令一步到位： $ wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda_8.0.61_375.26_linux-run 然后就是这个命令： $ sudo sh cuda_8.0.61_375.26_linux-run —override —silent —toolkit 安装的cuda在/usr/local/cuda下面。 3、安装Cudnn 8.0 v6.0，注意这里一定要装6.0的，不然后面会出现这种问题： （假设你装的是v5.0的）ImportError:libcudart.so.6.0:cannotopen shared object file:No such file or directory 如何安装和下载？ 用以下几条命令即可： 首先是： $ wget http://developer.download.nvidia.com/compute/redist/cudnn/v6.0/cudnn-8.0-linux-x64-v6.0-rc.tgz 意思是获取这个压缩文件。 然后，是这样的： $ sudo cp cudnn-8.0-linux-x64-v6.0-rc.tgz /usr/local/cuda $ cd /usr/local/cuda $ tar -xzvf cudnn-8.0-linux-x64-v6.0.tgz $ sudo cp cuda/include/cudnn.h /usr/local/cuda/include $ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64 $ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 最后就是，将以下两个路径加入到环境变量中。 具体做法是，输入 vim ~/.bashrc 输入i进入编辑模式，在末尾添加： export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64/ export LD_LIBRARY_PATH=”$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64” export CUDA_HOME=/usr/local/cuda 然后保存并退出。 最后，输入pip install tensorflow-gpu就可以大功告成。 三、SSH的安装SSH是指Secure Shell,是一种安全的传输协议，Ubuntu客户端可以通过SSH访问远程服务器。SSH分客户端openssh-client和openssh-server如果你只是想登陆别的机器的SSH只需要安装openssh-client（ubuntu有默认安装，如果没有则sudoapt-get install openssh-client），如果要使本机开放SSH服务就需要安装openssh-server。 安装服务端 Ubuntu缺省没有安装SSH Server，使用以下命令安装： $ sudo apt-get install openssh-server 然后确认sshserver是否启动了：（或用“netstat -tlp”命令） $ ps -e | grep ssh 如果只有ssh-agent那ssh-server还没有启动，需要/etc/init.d/ssh start，如果看到sshd那说明ssh-server已经启动了。 如果没有则可以这样启动： $ sudo/etc/init.d/ssh start SSH配置 ssh-server配置文件位于/etc/ssh/sshd_config，在这里可以定义SSH的服务端口，默认端口是22，你可以自己定义成其他端口号，如222。然后重启SSH服务： $ sudo /etc/init.d/sshresart 通过修改配置文件/etc/ssh/sshd_config，可以改ssh登录端口和禁止root登录。改端口可以防止被端口扫描。 $ sudo cp/etc/ssh/sshd_config /etc/ssh/sshd_config.original $ sudo chmod a-w /etc/ssh/sshd_config.original 编辑配置文件： gedit /etc/ssh/sshd_config 找到#Port 22，去掉注释，修改成一个五位的端口： Port 22333 找到#PermitRootLogin yes，去掉注释，修改为：PermitRootLogin no 配置完成后重启： $ sudo/etc/init.d/ssh restart SSH服务命令 停止服务：sudo /etc/init.d/ssh stop 启动服务：sudo /etc/init.d/ssh start 重启服务：sudo /etc/init.d/sshresart 断开连接：exit 登录：ssh root@192.168.0.100 $ ifconfig 可以查看IP地址 四、Ubuntu用户管理 添加用户 首先打开终端，输入：sudo adduser test，系统会提示以下信息： 到了这一步，新用户已经添加成功了，此时我们可以打 ls /home查看一下，可以看到test目录。 删除用户 $ sudo userdel test 删除成功后，系统无任何提示 添加用户权限 $ sudo vim /etc/sudoers 五、深度学习环境安装 TensorFlow的安装还是挺简单的，详细参考 OpenCV Python 3.5+OpenCV3.4.0 1、首先更新相关的package $ sudo apt-get update $ sudo apt-get install build-essential cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev 2、下载OpenCV的源码 $ wget -O opencv.zip https://github.com/Itseez/opencv/archive/3.4.0.zip $ unzip opencv.zip 3、编译安装 $ cd opencv-3.4.0 $ mkdir build $ cd build $ cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. $ make $ sudo make install 4、测试是否成功 $ python 123&gt;&gt; import cv2&gt;&gt; cv2.__version__'3.4.0' DELAY普通用户可以正常使用Python环境，但不能import TensorFlow？？？解决办法： $ vim ~/.bashrc 加入自己的环境变量 export LD_LIBRARY_PATH=/usr/local/cuda/lib64/ export CUDA_HOME=/usr/local/cuda $ source ~/.bashrc]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习笔记（二）-- 深度神经网络（DNN）优化之激活函数]]></title>
    <url>%2F2017%2F06%2F04%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89--%20%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88DNN%EF%BC%89%E4%BC%98%E5%8C%96%E4%B9%8B%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[激活函数（Activation function）维基百科的解释是，一个节点的激活函数定义了该节点在给定的输入或输入的集合下的输出。不要误解是指这个函数去激活什么，而是指如何把“激活的神经元的特征”通过函数把特征保留并映射出来，这是神经网络能解决非线性问题的关键。 因为神经网络的数学基础是处处可微的，所以选取的激活函数也要保证数据的输入和输出也是可微的，常见的激活函数有： 类型 取值范围 描述 tanh [-1, 1] 双切正切函数 sigmoid [0, 1] 采用S形函数 ReLu 大于0的留下，否则一律删除 简单而粗暴 神经网络中，运算特征是在不断进行循环计算，所以在每代的循环过程中，每个神经元的值也是在不断变化的。这就导致tanh特征相差明显时结果会更理想，在循环过程中会不断扩大特征效果显示出来，但有时候特征相差比较复杂或是相差不大时，就需要更加细粒度的分类去判断，这时sigmoid的效果就会好很多。而ReLu就是取max(0, x)，因为神经网络不断反复计算，实际上就变成了它在不断试探如何用一个大多数为0的矩阵刻画数据特征，因为稀疏特性（数据有冗余，但近似程度的最大保留数据特征可以用大多数元素为0的稀疏举证来实现）的存在，反而这种方法运算快效果又好，所以目前大多用ReLu代替sigmoid函数。 应用有这样一个需求，将下面的红蓝点进行正确的分类，其实就是二分类问题。 这时可以通过单层感知机算法找到一条合适的线性方程很容易的将它们划分开： 那如果给出的是这样的样本点呢： 可以看到该样本点不是线性可分的，一条直线怎么动都不可能完全正确的将三角形和圆形分开，有同学会说用多个感知器来进行组合，以求获得更大的分类问题，上图我们看下是否可行： 好的，我们已经得到多层感知机分类器了，那么分类能力是否就能将样本点正确分开呢，还是分析一下，我们得到的: y=w_{2-1}(w_{1-11}x_1+w_{1-21}x_2+b_{1-1})+w_{2-2}(w_{1-12}x_1+w_{1-22}x_2+b_{1-2})+w_{2-3}(w_{1-13}x_1+w_{1-23}x_2+b_{1-3})对这个公式合并同类项后得到： y=x_1(w_{2-1}w_{1-11}+w_{2-2}w_{1-12}+w_{2-3}w_{1-13})+x_2(w_{2-1}w_{1-21}+w_{2-2}w_{1-22}+w_{2-3}w_{1-23})+w_{2-1}b_{1-1}+w_{2-2}b_{1-2}+w_{2-3}b_{1-3}大家可以看出，不管它怎么组合，最终都是线性方程的组合，得到的分类器本质上还是一个线性方程，也就仍不能解决非线性问题。 就像下图，直线无论在平面上如何旋转，都不能将样本点分开 既然是非线性问题，总有线性方程不能正确分类的地方，上面的线性方程组合过程其实类似在做如下三条直线的组合，如图： 但如果在每一层叠加完后，加入一个激活函数，如图所示： 通过这个激活函数映射之后，输出很明显就是一个非线性函数，同理扩展到多个神经元组合，表达能力就会更强。 和上面线性组合相对应的非线性组合如下： 最后再通过最优化损失函数，不断训练优化，我们可以学习到能够正确分类的曲线，至于具体是什么样，谁也不知道。。maybe是下面的这个 end, 基于以上我们也就解释了一个观点—激活函数是用来加入非线性因素的，解决线性模型不能解决的问题。 more informationPerformance Analysis of Various Activation Functions inGeneralized MLP Architectures of Neural Networks]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test_my_site]]></title>
    <url>%2F2017%2F02%2F06%2Ftest-my-site%2F</url>
    <content type="text"></content>
      <tags>
        <tag>深度学习</tag>
        <tag>Java</tag>
      </tags>
  </entry>
</search>
