<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[New checkpoint format in TensorFlow]]></title>
      <url>/2018/01/04/New%20checkpoint%20format%20in%20TensorFlow/</url>
      <content type="html"><![CDATA[<p>从版本1.2.0之后，TensorFlow模型的checkpoint文件格式发生了变化，使restore之前老版本模型文件时报错。</p>
<h5 id="According-to-the-TensorFlow-v1-2-0-RC0’s-release-note"><a href="#According-to-the-TensorFlow-v1-2-0-RC0’s-release-note" class="headerlink" title="According to the TensorFlow v1.2.0 RC0’s release note:"></a>According to the TensorFlow v1.2.0 RC0’s release note:</h5><p>New checkpoint format becomes the default in tf.train.Saver. Old V1 checkpoints continue to be readable; controlled by the write_version argument, tf.train.Saver now by default writes out in the new V2 format. It significantly reduces the peak memory required and latency incurred during restore.</p>
<p>可以看出，tf.train.Saver函数有一个参数write_version，用于指定的新的checkpoint文件格式，新版本是V2，老版本是V1。保存的模型文件如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">old format(v1)</th>
<th style="text-align:center">new format(v2) </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">model-1000.ckpt</td>
<td style="text-align:center">model-1000.index</td>
</tr>
<tr>
<td style="text-align:center">model-1000.meta</td>
<td style="text-align:center">model-1000.meta</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">model-1000.data</td>
</tr>
</tbody>
</table>
</div>
<p>有些时候我们要使用旧版本的模型，该怎么办尼，查看<a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver" target="_blank" rel="noopener">Saver API</a>可以得知tf.train.Saver 的定义如下：</p>
<p>Saver类用于保存和恢复变量</p>
<p>Methods<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    var_list=<span class="keyword">None</span>,</span><br><span class="line">    reshape=<span class="keyword">False</span>,</span><br><span class="line">    sharded=<span class="keyword">False</span>,</span><br><span class="line">    max_to_keep=<span class="number">5</span>,  <span class="comment"># 要保留的最近checkpoint文件的最大数</span></span><br><span class="line">    keep_checkpoint_every_n_hours=<span class="number">10000.0</span>, <span class="comment"># 训练时每N小时保留一个检查点文件</span></span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    restore_sequentially=<span class="keyword">False</span>,</span><br><span class="line">    saver_def=<span class="keyword">None</span>,</span><br><span class="line">    builder=<span class="keyword">None</span>,</span><br><span class="line">    defer_build=<span class="keyword">False</span>,</span><br><span class="line">    allow_empty=<span class="keyword">False</span>,</span><br><span class="line">    write_version=tf.train.SaverDef.V2, <span class="comment"># 指定保存模型版本</span></span><br><span class="line">    pad_step_number=<span class="keyword">False</span>,</span><br><span class="line">    save_relative_paths=<span class="keyword">False</span>,</span><br><span class="line">    filename=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>所以，我们可以通过传指定参数来保存旧版本的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.core.protobuf <span class="keyword">import</span> saver_pb2</span><br><span class="line">...</span><br><span class="line">saver = tf.train.Saver(write_version=saver_pb2.SaverDef.V1)</span><br><span class="line">saver.save(sess, <span class="string">'./model.ckpt'</span>, global_step=step)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>但是这样保存模型会有提示警告信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">WARNING:tensorflow:*******************************************************</span><br><span class="line">WARNING:tensorflow:TensorFlow<span class="string">'s V1 checkpoint format has been deprecated.</span></span><br><span class="line"><span class="string">WARNING:tensorflow:Consider switching to the more efficient V2 format:</span></span><br><span class="line"><span class="string">WARNING:tensorflow: `tf.train.Saver(write_version=tf.train.SaverDef.V2)`</span></span><br><span class="line"><span class="string">WARNING:tensorflow:now on by default.</span></span><br><span class="line"><span class="string">WARNING:tensorflow:*******************************************************</span></span><br></pre></td></tr></table></figure>
<p>whatever, 但如果想获得更好的性能，还是推荐使用新的模型文件格式V2。</p>
<hr>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[实验室级别深度学习装机教程]]></title>
      <url>/2017/11/30/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E7%BA%A7%E5%88%AB%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%A3%85%E6%9C%BA%E6%95%99%E7%A8%8B/</url>
      <content type="html"><![CDATA[<h4 id="服务器配置"><a href="#服务器配置" class="headerlink" title="服务器配置"></a>服务器配置</h4><p>主板：华硕X99-aII</p>
<p>CPU:  Intel i7-6850K</p>
<p>显卡：华硕GTX 1080Ti OC * 2</p>
<p>内存：海盗船复仇者8G*4  2400MHZ</p>
<p>SSD: 三星SM961 256G</p>
<p>电源：海盗船AX1200i 全模组</p>
<p>机箱：海盗船AIR 540</p>
<p>散热器：海盗船H100i 一体式双排水冷</p>
<p><img src="/images/figures/2017-11-30-01.jpeg" alt="机箱"></p>
<h4 id="一、系统安装"><a href="#一、系统安装" class="headerlink" title="一、系统安装"></a>一、系统安装</h4><p>采用Ubuntu16.04，具体安装过程参见我之前的<br><a href="https://sconglee.github.io/posts/%E5%9C%A8UEFI%E6%A8%A1%E5%BC%8F%E4%B8%8B%E5%AE%89%E8%A3%85Ubuntu16.04%E4%B8%8EWindows%E5%8F%8C%E5%90%AF%E5%8A%A8" target="_blank" rel="noopener">博客</a></p>
<p>本次安装也出现了较双系统安装从未出现的问题：</p>
<ul>
<li>U盘启动盘不能被正常引导，开机后直接进入黑屏。。可以通过临时修改grub进入安装界面</li>
</ul>
<p>开机启动后按住Esc或是Right Shift进入以下界面</p>
<p><img src="/images/figures/2017-11-30-02.png" alt=""></p>
<p>选择第一项，按下e键就进入以下</p>
<p><img src="/images/figures/2017-11-30-03.png" alt=""></p>
<p>将图中的quiet splash 改为 nomodeset，然后按Ctrl+X进行boot，即可进入安装界面。</p>
<ul>
<li><p>以上方法仅仅是解决了第一次安装系统时不能正常引导的问题，当成功安装系统后重启，就会再次出现黑屏界面，所以要想从根本上解决问题，就必须更新grub。</p>
<p>$ sudo nano /etc/default/grub</p>
<p>添加nomodeset到GRUB_CMDLINE_LINUX_DEFAULT:</p>
<p>GRUB_DEFAULT=0</p>
<p>GRUB_HIDDEN_TIMEOUT=0</p>
<p>GRUB_HIDDEN_TIMEOUT_QUIET=true</p>
<p>GRUB_TIMEOUT=5</p>
<p>GRUB_DISTRIBUTOR=<code>lsb_release -i -s 2&gt; /dev/null || echo Debian</code></p>
<p>GRUB_CMDLINE_LINUX_DEFAULT=”quiet splash nomodeset”</p>
<p>GRUB_CMDLINE_LINUX=””</p>
<p>然后Ctrl+O保存，Ctrl+X退出</p>
<p>$ sudo update-grub</p>
</li>
</ul>
<p>这样就不会再出现开机黑屏的现象。</p>
<h4 id="二、安装CUDA、CUDNN-英伟达GPU加速框架"><a href="#二、安装CUDA、CUDNN-英伟达GPU加速框架" class="headerlink" title="二、安装CUDA、CUDNN 英伟达GPU加速框架"></a>二、安装CUDA、CUDNN 英伟达GPU加速框架</h4><h5 id="1、安装显卡驱动。"><a href="#1、安装显卡驱动。" class="headerlink" title="1、安装显卡驱动。"></a>1、安装显卡驱动。</h5><p>  先看一下自己的电脑对应的驱动版本。<br>  使用命令：ubuntu-drivers devices 会显示一个recommend的驱动版本。一般是三位数的，比如1080的显卡是384，950对应的是375。<br>  得到版本号之后，输入命令：</p>
<p>  $ sudo apt-get install nvidia-384 //最后三位是自己显卡驱动的版本号</p>
<h5 id="2、安装CUDA8-0点击下载"><a href="#2、安装CUDA8-0点击下载" class="headerlink" title="2、安装CUDA8.0点击下载"></a>2、安装CUDA8.0<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">点击下载</a></h5><p>  进去之后，选择Linux-&gt;x86_64-&gt;Ubuntu-&gt;16.04-&gt;runfile(local)<br>  即可下载，大概是一个多G，如果需要注册啥的，注册个就好。<br>  注意：以上只是一种下载CUDA8.0的方式。<br>  当然你可以通过命令一步到位：</p>
<p>  $ wget <a href="https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda_8.0.61_375.26_linux-run" target="_blank" rel="noopener">https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda_8.0.61_375.26_linux-run</a></p>
<p>  然后就是这个命令：</p>
<p>  $ sudo sh cuda_8.0.61_375.26_linux-run —override —silent —toolkit</p>
<p>  安装的cuda在/usr/local/cuda下面。</p>
<h5 id="3、安装Cudnn-8-0-v6-0，注意这里一定要装6-0的，不然后面会出现这种问题："><a href="#3、安装Cudnn-8-0-v6-0，注意这里一定要装6-0的，不然后面会出现这种问题：" class="headerlink" title="3、安装Cudnn 8.0 v6.0，注意这里一定要装6.0的，不然后面会出现这种问题："></a>3、安装Cudnn 8.0 v6.0，注意这里一定要装6.0的，不然后面会出现这种问题：</h5><p>   （假设你装的是v5.0的）ImportError:libcudart.so.6.0:cannotopen shared object file:No such file or directory<br>   如何安装和下载？ 用以下几条命令即可：<br>   首先是： </p>
<p>   $ wget <a href="http://developer.download.nvidia.com/compute/redist/cudnn/v6.0/cudnn-8.0-linux-x64-v6.0-rc.tgz" target="_blank" rel="noopener">http://developer.download.nvidia.com/compute/redist/cudnn/v6.0/cudnn-8.0-linux-x64-v6.0-rc.tgz</a><br>   意思是获取这个压缩文件。</p>
<p>   然后，是这样的：</p>
<p>   $ sudo cp cudnn-8.0-linux-x64-v6.0-rc.tgz /usr/local/cuda</p>
<p>   $ cd /usr/local/cuda</p>
<p>   $ tar -xzvf cudnn-8.0-linux-x64-v6.0.tgz</p>
<p>   $ sudo cp cuda/include/cudnn.h /usr/local/cuda/include</p>
<p>   $ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</p>
<p>   $ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</p>
<p>   最后就是，将以下两个路径加入到环境变量中。<br>   具体做法是，输入 vim  ~/.bashrc<br>   输入i进入编辑模式，在末尾添加：</p>
<p>   export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64/</p>
<p>   export LD_LIBRARY_PATH=”$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64”</p>
<p>  export CUDA_HOME=/usr/local/cuda<br>  然后保存并退出。</p>
<p>   最后，输入pip install tensorflow-gpu就可以大功告成。</p>
<h4 id="三、SSH的安装"><a href="#三、SSH的安装" class="headerlink" title="三、SSH的安装"></a>三、SSH的安装</h4><p>SSH是指Secure Shell,是一种安全的传输协议，Ubuntu客户端可以通过SSH访问远程服务器。SSH分客户端openssh-client和openssh-server如果你只是想登陆别的机器的SSH只需要安装openssh-client（ubuntu有默认安装，如果没有则sudoapt-get install openssh-client），如果要使本机开放SSH服务就需要安装openssh-server。</p>
<ul>
<li><p>安装服务端</p>
<p>  Ubuntu缺省没有安装SSH Server，使用以下命令安装：</p>
<p>  $ sudo apt-get install openssh-server</p>
<p>  然后确认sshserver是否启动了：（或用“netstat -tlp”命令）</p>
<p>  $ ps -e | grep ssh</p>
<p>  如果只有ssh-agent那ssh-server还没有启动，需要/etc/init.d/ssh start，如果看到sshd那说明ssh-server已经启动了。<br>  如果没有则可以这样启动：</p>
<p>  $ sudo/etc/init.d/ssh start</p>
</li>
<li><p>SSH配置</p>
<p>  ssh-server配置文件位于/etc/ssh/sshd_config，在这里可以定义SSH的服务端口，默认端口是22，你可以自己定义成其他端口号，如222。然后重启SSH服务：</p>
<p>  $ sudo /etc/init.d/sshresart<br>  通过修改配置文件/etc/ssh/sshd_config，可以改ssh登录端口和禁止root登录。改端口可以防止被端口扫描。</p>
<p>  $ sudo cp/etc/ssh/sshd_config /etc/ssh/sshd_config.original  </p>
<p>  $ sudo chmod a-w /etc/ssh/sshd_config.original<br>  编辑配置文件：</p>
<p>  gedit /etc/ssh/sshd_config<br>  找到#Port 22，去掉注释，修改成一个五位的端口：<br>  Port 22333</p>
<p>  找到#PermitRootLogin yes，去掉注释，修改为：PermitRootLogin no<br>  配置完成后重启：</p>
<p>  $ sudo/etc/init.d/ssh restart  </p>
</li>
<li><p>SSH服务命令</p>
<p>  停止服务：sudo /etc/init.d/ssh stop</p>
<p>  启动服务：sudo /etc/init.d/ssh start</p>
<p>  重启服务：sudo /etc/init.d/sshresart</p>
<p>  断开连接：exit</p>
<p>  登录：ssh root@192.168.0.100</p>
<p>  $ ifconfig 可以查看IP地址</p>
</li>
</ul>
<h4 id="四、Ubuntu用户管理"><a href="#四、Ubuntu用户管理" class="headerlink" title="四、Ubuntu用户管理"></a>四、Ubuntu用户管理</h4><ul>
<li><p>添加用户</p>
<p>首先打开终端，输入：sudo adduser test，系统会提示以下信息：</p>
</li>
</ul>
<p><img src="/images/figures/2017-11-30-04.png" alt=""></p>
<p>  到了这一步，新用户已经添加成功了，此时我们可以打 ls /home查看一下，可以看到test目录。</p>
<ul>
<li><p>删除用户</p>
<p>$ sudo userdel test </p>
<p>删除成功后，系统无任何提示</p>
</li>
<li><p>添加用户权限</p>
<p>$ sudo vim /etc/sudoers</p>
</li>
</ul>
<h4 id="五、深度学习环境安装"><a href="#五、深度学习环境安装" class="headerlink" title="五、深度学习环境安装"></a>五、深度学习环境安装</h4><ul>
<li>TensorFlow的安装还是挺简单的，<a href="http://tflearn.org/installation/" target="_blank" rel="noopener">详细参考</a></li>
<li><p>OpenCV</p>
<p>  Python 3.5+OpenCV3.4.0</p>
<p>  1、首先更新相关的package</p>
<p>  $ sudo apt-get update</p>
<p>  $ sudo apt-get install build-essential cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev</p>
<p>  2、下载OpenCV的源码</p>
<p>  $ wget -O opencv.zip <a href="https://github.com/Itseez/opencv/archive/3.4.0.zip" target="_blank" rel="noopener">https://github.com/Itseez/opencv/archive/3.4.0.zip</a></p>
<p>  $ unzip opencv.zip </p>
<p>  3、编译安装</p>
<p>  $ cd opencv-3.4.0  </p>
<p>  $ mkdir build  </p>
<p>  $ cd build</p>
<p>  $ cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. </p>
<p>  $ make </p>
<p>  $ sudo make install  </p>
<p>  4、测试是否成功</p>
<p>  $ python</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">import</span> cv2</span><br><span class="line">&gt;&gt; cv2.__version__</span><br><span class="line"><span class="string">'3.4.0'</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="DELAY"><a href="#DELAY" class="headerlink" title="DELAY"></a>DELAY</h4><p>普通用户可以正常使用Python环境，但不能import TensorFlow？？？解决办法：</p>
<p>$ vim ~/.bashrc</p>
<p>加入自己的环境变量</p>
<p>export LD_LIBRARY_PATH=/usr/local/cuda/lib64/</p>
<p>export CUDA_HOME=/usr/local/cuda</p>
<p>$ source ~/.bashrc</p>
<hr>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在UEFI模式下安装Ubuntu16.04与Windows双启动]]></title>
      <url>/2017/09/11/%E5%9C%A8UEFI%E6%A8%A1%E5%BC%8F%E4%B8%8B%E5%AE%89%E8%A3%85Ubuntu16.04%E4%B8%8EWindows%E5%8F%8C%E5%90%AF%E5%8A%A8/</url>
      <content type="html"><![CDATA[<p>从 Windows8 开始，微软用 UEFI（全称统一的可扩展固件接口，Unified Extensible Firmware Interface）取代了BIOS，UEFI有“安全启动”这个特点，引导程序只会启动那些得到 UEFI 固件签署的引导装载程序。此安全功能可以防止Rootkit类的恶意软件，并提供了额外的安全层。但它有一个缺点，如果你想在Win8/10的电脑上双引导Linux，安全机制会阻止这样做。</p>
<p>win7以下的系统可以跳过4、5步骤。</p>
<h4 id="如何判断电脑是否是uefi启动"><a href="#如何判断电脑是否是uefi启动" class="headerlink" title="如何判断电脑是否是uefi启动"></a>如何判断电脑是否是uefi启动</h4><p>按下win+r打开运行，输入msinfo32，确定，打开系统信息可以看到</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/1D837430D57E4A338DA5A5BA862ECBAA/7567" alt="UEFI"></p>
<h4 id="1、做个备份"><a href="#1、做个备份" class="headerlink" title="1、做个备份"></a>1、做个备份</h4><p>做个备份总是个好的选择，不至于丢失重要数据，方法很多不做介绍</p>
<h4 id="2、创建一个Ubuntu的USB启动盘"><a href="#2、创建一个Ubuntu的USB启动盘" class="headerlink" title="2、创建一个Ubuntu的USB启动盘"></a>2、创建一个Ubuntu的USB启动盘</h4><ul>
<li><a href="http://cn.ubuntu.com/download/" target="_blank" rel="noopener">按需下载ubuntu镜像</a></li>
</ul>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/C8A5EE57E56B4336A39FDAADB106230A/7562" alt="ubuntu"></p>
<ul>
<li><p>使用<a href="https://cn.ultraiso.net/xiazai.html" target="_blank" rel="noopener">UltraISO下载</a>写入ubuntu镜像</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/6C000B58E6744E9B8E72FA88E4D525F4/7611" alt="制作启动盘"></p>
<p><a href="http://jingyan.baidu.com/article/19020a0a396b6e529d2842cb.html" target="_blank" rel="noopener">详细过程可参见</a></p>
</li>
</ul>
<h4 id="3、为Ubuntu划分一块安装分区"><a href="#3、为Ubuntu划分一块安装分区" class="headerlink" title="3、为Ubuntu划分一块安装分区"></a>3、为Ubuntu划分一块安装分区</h4><p>假设你有一个全新的系统，我们要做的第一件事是创建一个分区来安装Linux。你可以通过在控制面板中搜索‘磁盘’找到磁盘管理工具。</p>
<p>在磁盘管理工具中，右键点击你想划分并缩小的卷：</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/E6BBC5493B114F419422759037FB6555/7561" alt="磁盘"></p>
<p>缩小后出现的未分配空间就放在那里好了，不用对其分区和格式化，我们会在安装Ubuntu时用到它。</p>
<h4 id="4、在Windows中禁用快速启动-可选"><a href="#4、在Windows中禁用快速启动-可选" class="headerlink" title="4、在Windows中禁用快速启动 [可选]"></a>4、在Windows中禁用快速启动 [可选]</h4><p>为了实现快速启动，Windows8/10引进了叫做“快速启动”的新特性。尽管不强制要求，最好还是将其禁用。</p>
<p>步骤如下：打开控制面板 &gt; 硬件与声音 &gt; 电源选项 &gt; 选择电源按钮的功能 &gt; 更改当前不可用的设置，取消选中启用快速启动（推荐）。</p>
<h4 id="5、禁用Windows的安全启动（secure-boot）"><a href="#5、禁用Windows的安全启动（secure-boot）" class="headerlink" title="5、禁用Windows的安全启动（secure boot）"></a>5、禁用Windows的安全启动（secure boot）</h4><p>这是非常重要的步骤，为了实现Windows和Linux的双启动，我们必须在UEFI中禁用安全启动（secure boot）。</p>
<p>虽然在 BIOS 时代，访问BIOS是相当简单的，在启动的时候按F10或F12键即可。但是在 UEFI 的世界里，就不一样了。要访问 UEFI 设置，你就必须进入到 Windows 中去，让我们来看看如何在 Windows 8 中访问 UEFI 设置来禁用安全启动。</p>
<ul>
<li>进入PC设置，点击 Windows+I 按钮进入 Windows 设置界面，点击更新和安全选项</li>
<li>进入高级启动</li>
</ul>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/8A51F3F108F94B988FEC572715D79C29/7565" alt="高级启动"></p>
<p>这之后并不会立刻重新启动，而是会在下一步看到一些选项。</p>
<ul>
<li>进入UEFI设置</li>
</ul>
<p>—&gt;疑难解答<br><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/0E4DA0A7691F44EB85728616FEA063C7/7557" alt="u1"><br>—&gt;高级选项<br><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/56A3C3368CE94BA6AE034034F3792231/7558" alt="u2"><br>—&gt;UEFI固件设置<br><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/77A0D0ACB0CF4EC79A26D037B8E6FB0A/7559" alt="u3"></p>
<p>接下来，在UEFI设置里，点击重新启动按钮重新启动您的系统，就会看到类似BIOS一样的界面。</p>
<ul>
<li>在UEFI中禁用安全启动</li>
</ul>
<p>这个时候，你一定已经启动到UEFI工具界面，移动到启动选项卡，在那里你会发现安全引导选项被设置为启用，将其改为Disabled并保存即可。</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/813823F142F64C969682F47EA4D8ECF4/7612" alt="boot"></p>
<p>接下来将正常引导到Windows，现在就支持双启动 Windows8/10 与 Ubuntu 或其它 Linux 操作系统了。</p>
<h4 id="6、安装Ubuntu"><a href="#6、安装Ubuntu" class="headerlink" title="6、安装Ubuntu"></a>6、安装Ubuntu</h4><p>点击重新启动并按住shift，在类似UEFI的界面上选择从USB启动的选项。</p>
<p>当你用USB启动盘启动后，你会看到试用（try）或者安装（install）Ubuntu的选择，这里要点击“安装”。</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/9C85E53A9D494CBD8302EC40C04CCA83/7553" alt="b1"></p>
<p>安装窗口中你需要注意的是安装类型（Installation Type）。选择这里的其它选项（Something else）：</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/32DF7031A651421CA56EEA61A197133E/7556" alt="b2"></p>
<p>我们将用之前创建的分区来创建根分区（/），交换空间（Swap）以及家目录（Home）。选择空闲（free space）然后点击加号（+）。</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/3A285F2F33C74E4481760DA9AF893969/7552" alt="b3"></p>
<p>根分区 / 10到20GB空间就足够了，选择大小（Size），然后选择Ext4作为文件系统以及 /（意思是根）作为挂载点（Mount point）。</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/183EDE6916DD4644A1E4ABC35AC4BE86/7564" alt="b4"></p>
<p>点击确定会回到分区界面，下一步我们创建交换空间（Swap）。像之前一样，再次点击加号（+），这次我们选择作为交换空间（Swaparea），建议的交换空间大小是物理内存的两倍。</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/6B323E7D7C2C4B019DE21FD3FF6E723C/7566" alt="b5"></p>
<p>以同样的方式创建家目录（Home）。给它分配最大的空间（实际上是给它分配剩余的所有空间），因为这是你会用来存储音乐，图片以及下载的文件的位置。</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/E1E9085112214C52B841E6D573FFE0C9/7563" alt="b6"></p>
<p>分配好了根分区（ / ），交换空间（Swap）和家目录（Home）之后，点击现在安装（Install Now）：</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/25AAD2C4FA864D0E80F9BE1DF10A82FE/7554" alt="b7"></p>
<p>接下来的就是设置用户名密码等等，基本上就是只需点击下一步。</p>
<p><img src="http://note.youdao.com/yws/public/resource/84292c2be23d0e571d23baaad9e216e4/xmlnote/365602FBCB1140EABE1C179521122C91/7609" alt="b8"></p>
<p>一旦安装完成，重新启动电脑，你应该会看到紫色的grub欢迎界面，表明Ubuntu和Windows 8的双启动模式安装成功了。</p>
<hr>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ImageNet Classification with Deep Convolutional Neural Networks 笔记]]></title>
      <url>/2017/08/08/ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%20%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h4 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h4><p>本文训练了一个深度卷积神经网络（下文称CNNs）来将ILSVRC-2010中1.2M（注：本文中M和K均代表 百万/千 个数量）的高分辨率图像（注：ImageNet目前共包含大约22000类，15兆左右的标定图像，ILSVRC-2010为其中一个常用的数据集）数据分为1000类。测试结果，Top-1和Top-5的错误率分别为37.5%和17%，优于当时最优的水平。</p>
<p>后来作者利用该种模型的变体参与了ILSVRC-2012（ImageNet Large Scale Visual Recognition Challenge）比赛，以Top-5错误率15.3%遥遥领先亚军的26.2%。最后文章发表于NIPS 2012（Neural Information Processing Systems），目的是Hinton与其学生为了回应别人对于deep learning的质疑而将deep learning用于ImageNet（图像识别目前最大的数据库）上。</p>
<h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p><img src="/images/figures/2017-08-08-01.jpg" alt="网络结构"></p>
<p>该神经网络包含60M参数和650K神经元，用5个卷积层（其中某些层与亚采样层连接）、三个全连接层（包括一个1K门的输出层）。为使训练更快，文章采用非饱和神经元，并利用了一个高效的GPU应用进行卷积运算。在全连接层中，为避免过拟合，文章采用了一种叫做“dropout”的方法。</p>
<p>由上图可以总结Alex-net:Input(224,224,3)→96F1(11,11,3,s=4)→LRN→max-P1(3,3,2)→256F2(3,3,96)→LRN→max-P2(3,3,2)→384F3(3,3,192)→384F4(3,3,192)→256F5(3,3,192)→max-P3(3,3,2)→4096fc1→4096fc2→Classifer1000</p>
<h4 id="突出贡献"><a href="#突出贡献" class="headerlink" title="突出贡献"></a>突出贡献</h4><ul>
<li>基于ILSVRC-2010和ILSVRC-2012的子集训练了一个最大的CNNs模型并达到了该数据集中的当前最优结果。</li>
<li>写了一个在GPU中高度优化的二维卷积的实现以及其他一些固有的CNNs的训练操作（代码：<a href="http://code.google.com/p/cuda-convnet/）。" target="_blank" rel="noopener">http://code.google.com/p/cuda-convnet/）。</a></li>
<li>网络中包括了大量不常见和新的特征来提升性能，减少训练时间。</li>
<li>文章网络的大小导致过拟合成为一个严重的问题，所以文章采取了一些有效的技术来预防过拟合。（详见：四 降低过拟合）</li>
<li>把Hinton 10年提出的用于改善RBM性能的ReLu引入CNN中，使得ReLu成为以后深度网络普遍使用的非线性函数，从而替代了经典的sigmoid，tanh函数。</li>
</ul>
<p>最终网络大小主要受限于GPU的内存和训练时间。实验证明，本网络在有两个GTX 580 3GB GPU的机器上训练了5-6天。实验结果显示，如果GPU更快或数据集更大，实验结果将会更好。</p>
<h4 id="学而时习"><a href="#学而时习" class="headerlink" title="学而时习"></a>学而时习</h4><ul>
<li>ReLu非线性不饱和函数（相对于tanh或sigmoid函数，在x非常大或非常小时，函数输出基本不变，故称为饱和函数），又称为扭曲线性函数，不但保留了非线性的表达能力，而且还具有线性性质（正值部分），相比tanh和sigmoid函数在误差反向传递时，不会有由于非线性引起的梯度弥散现象（顶层误差较大，由于逐层递减误差传递，引起底层误差很小，导致深度网络底层权值更新量很小，从而导致深度网络局部最优）。</li>
<li>数据增强—-图像平移，水平翻转。操作过程是：给定一个原始图像a 1080 <em> 780，要得到224 </em> 224的训练样本，可以先把a缩小到b 360 <em> 256，然后在b的中心取256 </em> 256的图片得到c，然后再在c上随机提取224 * 224的图片作为训练样本，最后再结合图像水平翻转来增加样本从而达到数据增益。</li>
<li>数据增强—-调整RGB通道强度。个人理解也就是对图片进行主成份分析，然后给RGB分别加上一个量，这样可以成倍增加已有主成分，这种技术利用了捕捉自然图像的一个重要性质，即改变颜色和像素值，物体是不变的。</li>
<li>使用重叠pooling（filter size &gt; strides），不容易过拟合。</li>
<li>测试方法—-在待测图片的四角和中心提取5个片段，在水平翻转后形成10个样本，输入网络结果求平均。</li>
</ul>
<h4 id="困惑吧啦吧啦"><a href="#困惑吧啦吧啦" class="headerlink" title="困惑吧啦吧啦"></a>困惑吧啦吧啦</h4><ul>
<li>Alex-net网络中layer-3的filter把layer-2的所有特征图作为输入，而其它卷积层，只从同一个GPU内的上一层特征图作为输入。为什么layer-3把layer-2的全部特征图作为输入而且它层却不这样，并没有给出解释理论依据，只是说通过交叉验证实验得来。</li>
<li>无论在什么初始值下，两个GPU也就是两个相同的结构，学习到的卷积核不一样，GPU1学习到的特征大多数颜色不明确，GPU2学习到的特征大部分具备颜色。</li>
<li>局部加权回归-待续</li>
</ul>
<hr>
<h4 id="more-information"><a href="#more-information" class="headerlink" title="more information"></a>more information</h4><p><a href="http://pan.baidu.com/s/1miifbXe" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<hr>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow学习笔记（七）-- 基于CNN的目标分类框架]]></title>
      <url>/2017/08/03/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89--%20%E5%9F%BA%E4%BA%8ECNN%E7%9A%84%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB%E6%A1%86%E6%9E%B6/</url>
      <content type="html"><![CDATA[<p>一个基本的目标分类框架包括以下三个方面，还是直接上图吧。。</p>
<p><img src="/images/figures/2017-08-03-01.jpg" alt="预处理流程"></p>
<h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><p>TensorFlow提供了统一格式TFRecord来存储所有训练数据，当数据量较大时，也可以将数据写入多个TFRecord文件中，另外TensorFlow对从文件列表中读取数据也提供了很好的支持。具体写入和读取过程不做具体说明。</p>
<p>下图是训练数据的预处理流程</p>
<p><img src="/images/figures/2017-08-03-02.jpg" alt="预处理流程"></p>
<h4 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h4><p>得到整理好的batch数据后，就会被feed进神经网络中，后续整个流程如下图：</p>
<p><img src="/images/figures/2017-08-03-03.jpg" alt="卷积网络"></p>
<p>通过该框架就可以完整的实现图像的训练与识别。</p>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow学习笔记（六）-- 卷积神经网络（CNN）反向传播算法]]></title>
      <url>/2017/07/31/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89--%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<h4 id="回顾DNN反向传播算法"><a href="#回顾DNN反向传播算法" class="headerlink" title="回顾DNN反向传播算法"></a>回顾DNN反向传播算法</h4><p>在前面已经对DNN反向传播算法做了介绍，这里只给出简单推导过程</p>
<p><img src="/images/figures/2017-07-31-01.jpg" alt="DNN反向传播1"></p>
<p><img src="/images/figures/2017-07-31-02.jpg" alt="DNN反向传播2"></p>
<p>有了$W$,$b$梯度表达式，就可以用梯度下降法来优化$W$,$b$，求出最终的所有$W$,$b$的值。</p>
<p>现在我们想把同样的思想用到CNN中，很明显，CNN有些不同的地方，不能直接去套用DNN的反向传播算法的公式。</p>
<h4 id="CNN反向传播算法思想"><a href="#CNN反向传播算法思想" class="headerlink" title="CNN反向传播算法思想"></a>CNN反向传播算法思想</h4><p>要想套用DNN的反向传播算法到CNN，要解决以下问题：</p>
<ul>
<li>池化层在前向传播的时候，对输入进行了压缩，那么我们现在需要向前反向推导$\delta^{l-1}$，这个推导方法和DNN完全不同。</li>
<li>卷积层是通过张量卷积，或者说若干个矩阵卷积求和而得的当前层的输出，这和DNN很不相同，DNN的全连接层是直接进行矩阵乘法得到当前层的输出。这样在卷积层反向传播的时候，上一层的$\delta^{l-1}$递推计算方法肯定有所不同。</li>
<li>对于卷积层，由于W使用的运算是卷积，那么从$\delta^l$推导出该层的所有卷积核的$W$,$b$的方式也不同。</li>
</ul>
<h4 id="CNN反向传播过程"><a href="#CNN反向传播过程" class="headerlink" title="CNN反向传播过程"></a>CNN反向传播过程</h4><p>推导过程也就是依次解决以上问题的过程，需要注意的是，由于卷积层可以有多个卷积核，各个卷积核的处理方法是完全相同且独立的，为了简化算法公式的复杂度，我们下面提到卷积核都是卷积层中若干卷积核中的一个。</p>
<p><img src="/images/figures/2017-07-31-03.jpg" alt="CNN反向传播"></p>
<p>可以看出最后的结果和DNN类似，不同点在于对含有卷积核的式子求导时，卷积核被旋转了180度，即式子中的$rot180()$，翻转180度的意思是上下翻转一次，接着左右翻转一次，而在DNN中这里只是矩阵的转置，那么为什么呢，接下来就以一个简单的例子来说明。</p>
<p><img src="/images/figures/2017-07-31-04.jpg" alt="卷积核翻转1"></p>
<p><img src="/images/figures/2017-07-31-05.jpg" alt="卷积核翻转2"></p>
<p>最后的矩阵表示，为了符合梯度计算，就在误差矩阵周围填充了一圈0，此时我们将卷积核翻转后和反向传播的梯度误差进行卷积，就得到了前一次的梯度误差，也就直观说明了在对含有卷积的式子求导时，卷积核需要翻转180度。</p>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow Q&A]]></title>
      <url>/2017/07/13/TensorFlow%20Q&amp;A/</url>
      <content type="html"><![CDATA[<h4 id="1-TensorFlow供给数据（Feeding）错误，具体信息如下图："><a href="#1-TensorFlow供给数据（Feeding）错误，具体信息如下图：" class="headerlink" title="1. TensorFlow供给数据（Feeding）错误，具体信息如下图："></a>1. TensorFlow供给数据（Feeding）错误，具体信息如下图：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    mnist = getMnist()</span><br><span class="line">    img, label = read_tfrecord()</span><br><span class="line">    img_batch, label_batch = tf.train.shuffle_batch([img, label],</span><br><span class="line">                                                   batch_size=<span class="number">100</span>,</span><br><span class="line">                                                   num_threads=<span class="number">2</span>,</span><br><span class="line">                                                   capacity=<span class="number">1000</span>,</span><br><span class="line">                                                   min_after_dequeue=<span class="number">700</span>)</span><br><span class="line">    init = tf.initialize_all_variables()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> sess.as_default():</span><br><span class="line">        sess.run(init)</span><br><span class="line">        coord = tf.train.Coordinator()</span><br><span class="line">        threads = tf.train.start_queue_runners(sess=sess, coord=coord)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">            img_np, label_np = sess.run([img_batch, label_batch])</span><br><span class="line">            sess.run(train_step, feed_dict=&#123;x:img_np, y_:label_np&#125;)</span><br><span class="line">            <span class="keyword">if</span> (i % <span class="number">100</span> == <span class="number">0</span>):</span><br><span class="line">                print(<span class="string">"step %4d "</span> %i)</span><br><span class="line">    print(<span class="string">"Accuracy on testdata:"</span>, sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, y_:tf.cast(tf.arg_max(mnist.test.labels, <span class="number">1</span>), tf.int32)&#125;))</span><br><span class="line">    coord.request_stop()</span><br><span class="line">    coord.join(threads)</span><br></pre></td></tr></table></figure>
<p><img src="/images/figures/2017-07-13-01.png" alt="error1"></p>
<p>报错意思是被feed进的值不能是一个张量，而应该是Python scalars，strings， lists， 或是arrays中的一种，定位问题出现在倒数第三行的print语句的tf.cast(tf.arg_max(mnist.test.labels, 1), tf.int32)，该值是个tensor，可以改为以下的方式注入参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ys = sess.run(tf.cast(tf.arg_max(mnist.test.labels, <span class="number">1</span>), <span class="string">"int32"</span>))       </span><br><span class="line">print(<span class="string">"Accuracy on testdata:"</span>, sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, y_:ys&#125;))</span><br></pre></td></tr></table></figure>
<p>tips：Tensorflow的数据供给机制允许在运算图中将数据注入到任一张量中，但必须通过run()或者eval()函数输入feed_dict参数，才可以启动运算过程。而且设计placeholder节点的意图就是为了提供数据供给的方法，该节点在声明时是未初始化的，也不包含数据，所以如果没有给它feed数据，则Tensorflow运算的时候会产生错误。</p>
<h4 id="2-计算图的误用"><a href="#2-计算图的误用" class="headerlink" title="2. 计算图的误用"></a>2. 计算图的误用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># num_to_char()是自定义的函数，用于将给定的value转化为key</span></span><br><span class="line">preValue = num_to_char(tf.arg_max(tf.nn.softmax(y), <span class="number">1</span>))</span><br><span class="line"><span class="keyword">print</span> <span class="string">"The prediction value is:"</span>, sess.run(preValue, feed_dict=&#123;x:testPicArr&#125;)</span><br></pre></td></tr></table></figure>
<p><img src="/images/figures/2017-07-13-02.png" alt="error2"></p>
<p>可以看到报错提示是无效的参数类型，该处run()中的preValue应该是一个tensor。而且num_to_char(tf.arg_max(tf.nn.softmax(y), 1))也不是一个有效的值，因为tf.arg_max(tf.nn.softmax(y), 1)也是tensor，没有在session中run之前，它仅仅是一个计算图的节点符号，没有实际值的意义，所以num_to_char()返回的是None。可以做如下修改</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># num_to_char()是自定义的函数，用于将给定的value转化为key</span></span><br><span class="line">preValue = tf.arg_max(tf.nn.softmax(y), <span class="number">1</span>)</span><br><span class="line">value = sess.run(preValue, feed_dict=&#123;x:testPicArr&#125;)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"index is: "</span> , value, <span class="string">"The prediction value is:"</span>, num_to_char(value)</span><br></pre></td></tr></table></figure>
<p>tips：Tensorflow是一个编程系统，仅仅使用图来表示计算任务，图中的每个节点有0或多个tensor，多个节点组成的图描述了计算的过程，为了进行计算，该图必须在会话里启动，从而返回对应的值类型。</p>
<h4 id="3-tf-nn-conv2d-中参数padding-method的使用"><a href="#3-tf-nn-conv2d-中参数padding-method的使用" class="headerlink" title="3. tf.nn.conv2d()中参数padding_method的使用"></a>3. tf.nn.conv2d()中参数padding_method的使用</h4><p>这里引用Stack Overflow中的一个解释：</p>
<p>SAME：means that the output feature map has the same spatial dimensions as the input feature map. Zero padding is introduced to make the shapes match as needed, equally on every side of the input map.</p>
<p>VALID: means no padding.</p>
<p>追踪源码至tensorflow/tensorflow/python/ops/nn_ops.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolution</span><span class="params">(input, filter, padding, strides=None, dilation_rate=None, </span></span></span><br><span class="line"><span class="function"><span class="params">                           name=None, data_format=None)</span>:</span></span><br><span class="line">                           </span><br><span class="line">    Args:</span><br><span class="line">      padding: A string, either `<span class="string">"VALID"</span>` <span class="keyword">or</span> `<span class="string">"SAME"</span>`. The padding algorithm.</span><br><span class="line">      dilation_rate: Optional, Specifies the filter upsampling/input downsampling rate, usually dilation_rate=<span class="number">1.</span></span><br><span class="line">     </span><br><span class="line">    Returns:</span><br><span class="line">        If padding == <span class="string">"SAME"</span>:</span><br><span class="line">            output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])</span><br><span class="line">        If padding == <span class="string">"VALID"</span>:</span><br><span class="line">            output_spatial_shape[i] =ceil((input_spatial_shape[i] -</span><br><span class="line">              (spatial_filter_shape[i]<span class="number">-1</span>) * dilation_rate[i]) / strides[i]).</span><br></pre></td></tr></table></figure></p>
<p>整理下就是，对于“VALID”，输出的形状计算如下：</p>
<script type="math/tex; mode=display">new\_shape_{i}=\lceil\dfrac{(W_{i}-F_{i}+1)}{S_{i}}\rceil</script><p>对于“SAME”，输出的形状计算如下：</p>
<script type="math/tex; mode=display">new\_shape_{i}=\lceil\dfrac{W_{i}}{S_{i}}\rceil</script><p>其中，$i$是对应的维度，$W$为输入的size，$F$是filter的size，$S$是步长，$\lceil\rceil$是向上取整符号。</p>
<h4 id="4-softmax-cross-entropy-with-logits和sparse-softmax-cross-entropy-with-logits的区别"><a href="#4-softmax-cross-entropy-with-logits和sparse-softmax-cross-entropy-with-logits的区别" class="headerlink" title="4. softmax_cross_entropy_with_logits和sparse_softmax_cross_entropy_with_logits的区别"></a>4. softmax_cross_entropy_with_logits和sparse_softmax_cross_entropy_with_logits的区别</h4><p>官方给出的解释还是简单明了的</p>
<ul>
<li>For sparse_softmax_cross_entropy_with_logits, labels must have the shape [batch_size] and the dtype int32 or int64. Each label is an int in range [0, num_classes-1].</li>
<li>For softmax_cross_entropy_with_logits, labels must have the shape [batch_size, num_classes] and dtype float32 or float64.</li>
<li>Labels used in softmax_cross_entropy_with_logits are the one hot version of labels used in sparse_softmax_cross_entropy_with_logits.</li>
</ul>
<p>两个函数返回值是一样的，但都不是一个数，而是一个向量，如果要求loss，则要做一步tf.reduce_mean()操作，即对向量求均值。</p>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow学习笔记（五）-- 深度神经网络（DNN）反向传播算法（BP）]]></title>
      <url>/2017/06/27/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89--%20%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88DNN%EF%BC%89%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%EF%BC%88BP%EF%BC%89/</url>
      <content type="html"><![CDATA[<h4 id="DNN反向传播算法-for-what"><a href="#DNN反向传播算法-for-what" class="headerlink" title="DNN反向传播算法 for what ?"></a>DNN反向传播算法 for what ?</h4><p>在了解DNN的反向传播算法前，我们要先搞清楚反向传播算法是用来解决什么问题的，也就是说，什么时候我们需要使用这个反向传播算法。</p>
<p>这里有个监督学习的一般问题，假设我们有m个训练样本：${(x<em>1,y_1),(x_2,y_2),…,(x_m,y_m)}$，其中$x$ 是输入向量，特征维度为<script type="math/tex">n\_in</script>，而$y$是输出向量，特征维度是<script type="math/tex">n\_out</script>。现在我们需要利用这个样本训练出一个模型，当有一个新的测试样本$(x</em>{test},?)$，我们可以预测$y_{test}$向量的输出。</p>
<p>如果我们采用DNN的模型，即输入层有<script type="math/tex">n\_in</script>个神经元，输入层有<script type="math/tex">n\_out</script>个神经元，再加上一些含有若干神经元的隐藏层。此时我们还需要找到合适的所有隐藏层和输出层对应的线性系数矩阵$W$，偏置向量$b$，让所有输入的训练样本计算出的输出尽可能等于或很接近样本的输出。那么问题就来了，怎么找到合适的参数尼？</p>
<p>了解传统机器学习算法的优化过程的话，就会很容易想到可以用一个合适的损失函数来度量训练样本的输出损失，接着对这个损失函数进行优化求最小化的极值，对应的一系列线性系数矩阵$W$，以及偏置向量$b$即为我们的最终结果。在DNN中，损失函数优化极值求解的过程最常见的一般是通过梯度下降法来一步步迭代完成的，当然也可以是其他的迭代方法比如牛顿法与拟牛顿法。</p>
<p>可以知道，<strong>对DNN的损失函数用梯度下降法进行迭代优化求极值的过程就是我们所说的反向传播算法</strong>。</p>
<h4 id="DNN反向传播算法的基本思路"><a href="#DNN反向传播算法的基本思路" class="headerlink" title="DNN反向传播算法的基本思路"></a>DNN反向传播算法的基本思路</h4><p>为便于通俗直观的理解该算法，这里我定义了个三层网络，第0层输入层，第一层隐藏层，第二层输出层，并且每个节点没有偏置（有偏置原理完全一样），激活函数为sigmod，其中使用到的符号说明如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$W_{ab}$</td>
<td style="text-align:center">代表的是节点a到节点b的权重</td>
</tr>
<tr>
<td style="text-align:center">$y_a$</td>
<td style="text-align:center">代表的是节点a的输出值</td>
</tr>
<tr>
<td style="text-align:center">$Z_a$</td>
<td style="text-align:center">代表的是节点a的输入值</td>
</tr>
<tr>
<td style="text-align:center">$C$</td>
<td style="text-align:center">最终损失函数</td>
</tr>
<tr>
<td style="text-align:center">$f(x)=\dfrac{1}{1+e^{-x}}$</td>
<td style="text-align:center">节点激活函数</td>
</tr>
<tr>
<td style="text-align:center">$W1$</td>
<td style="text-align:center">左边字母，右边数字，代表第几层的权重</td>
</tr>
</tbody>
</table>
</div>
<p>对应的网络如下：</p>
<p><img src="/images/figures/2017-06-27-01.png" alt="神经网络"></p>
<script type="math/tex; mode=display">X=Z_0=\left[
         \begin{matrix}
         0.35 \\
         0.9 
         \end{matrix}
         \right]</script><script type="math/tex; mode=display">y_{out}=0.5</script><script type="math/tex; mode=display">W0=\left[
     \begin{matrix}
     w_{31} & w_{32} \\
     w_{41} & w_{42}
     \end{matrix}
     \right]
    =\left[
    \begin{matrix}
    0.1 & 0.8 \\
    0.4 & 0.6 
    \end{matrix}
    \right]</script><script type="math/tex; mode=display">W1=\left[
     \begin{matrix}
     w_{53} & w_{54}
     \end{matrix}
     \right]
    =\left[
    \begin{matrix}
    0.3 & 0.9
    \end{matrix}
    \right]</script><p>首先我们先走一遍<strong>正向传播过程</strong>，如下推导:</p>
<script type="math/tex; mode=display">Z_1=\left[
      \begin{matrix}
      Z_3 \\
      Z_4 
      \end{matrix}
      \right]
    =W0*X
    =\left[
     \begin{matrix}
     w_{31} & w_{32} \\
     w_{41} & w_{42}
     \end{matrix}
     \right]*
     \left[
     \begin{matrix}
     x_1 \\
     x_2
     \end{matrix}
     \right]
    =\left[
     \begin{matrix}
     w_{31}*x_1 + w_{32}*x_2 \\
     w_{41}*x_1 + w_{42}*x_2
     \end{matrix}
     \right]
    =\left[
     \begin{matrix}
     0.755 \\
     0.68
     \end{matrix}
     \right]</script><p>那么隐藏层的输出为：</p>
<script type="math/tex; mode=display">y_1=\left[
     \begin{matrix}
     y_3 \\
     y_4
     \end{matrix}
     \right]
    =f(Z_1)
    =f(\left[
     \begin{matrix}
     0.755 \\
     0.68
     \end{matrix}
     \right])
    =f(\left[
     \begin{matrix}
     0.680 \\
     0.663
     \end{matrix}
     \right])</script><p>同理可以得到：</p>
<script type="math/tex; mode=display">Z_2=W1*y_1
     =\left[
     \begin{matrix}
     W_{53} & W_{54}
     \end{matrix}
     \right]*
    \left[
     \begin{matrix}
     y_3 \\
     y_4
     \end{matrix}
     \right]
    =\left[
     \begin{matrix}
     0.801
     \end{matrix}
     \right]</script><script type="math/tex; mode=display">y_2=f(Z_2)
     =f(\left[
     \begin{matrix}
     0.801
     \end{matrix}
     \right])
    =\left[
     \begin{matrix}
     0.690
     \end{matrix}
     \right]</script><p>那么最终的损失为：</p>
<script type="math/tex; mode=display">C=\dfrac{1}{2}(0.690-0.5)^2=0.01805</script><p>对于这个损失值，我们当然是希望这个值越小越好。这也是我们进行多次训练，调节参数的目的，在这个训练的过程中就用到了我们的反向传播算法，实际上反向传播就是梯度下降法中链式法则的使用。</p>
<p>下面是<strong>反向传播的推导过程</strong></p>
<p>根据公式，我们有：</p>
<script type="math/tex; mode=display">\begin{cases}
  Z_2=W_{53}*y_3+W_{54}*y_4 \\
  y_2=f(Z_2) \\
  C=\dfrac{1}{2}(y_2-y_{out})^2 
  \end{cases}</script><p>这个时候我们需要求出$C$对$W$的偏导，则根据链式法则有：</p>
<script type="math/tex; mode=display">\dfrac{\partial C}{\partial W_{53}}=\dfrac{\partial C}{\partial y_5}*\dfrac{\partial y_5}{\partial Z_5}*\dfrac{\partial Z_5}{\partial W_{53}}

=(y_5-y_{out})*f(Z_2)*(1-f(Z_2))*y_3

=0.02763</script><p>同理有:</p>
<script type="math/tex; mode=display">\dfrac{\partial C}{\partial W_{54}}=\dfrac{\partial C}{\partial y_5}*\dfrac{\partial y_5}{\partial Z_5}*\dfrac{\partial Z_5}{\partial W_{54}}

=(y_5-y_{out})*f(Z_2)*(1-f(Z_2))*y_4

=0.02711</script><p>至此我们已经求出最后一层的参数偏导了，继续向前链式推导：</p>
<p>我们现在还需要求出： $W<em>{31},W</em>{32},W<em>{41},W</em>{42}$ </p>
<p>已知：</p>
<script type="math/tex; mode=display">\begin{cases}
  Z_3=W_{31}*x_1+W_{32}*x_2 \\
  y_3=f(Z_3) \\
  Z_5=W_{53}*y_3+W_{54}*y_4 \\
  y_5=f(Z_5) \\
  C=\dfrac{1}{2}(y_5-y_{out})^2
  \end{cases}</script><p>则：</p>
<script type="math/tex; mode=display">\dfrac{\partial C}{\partial W_{31}}=\dfrac{\partial C}{\partial y_5}*\dfrac{\partial y_5}{\partial Z_5}*\dfrac{\partial Z_5}{\partial y_3}*\dfrac{\partial y_3}{\partial Z_3}*\dfrac{\partial Z_3}{\partial Z_{31}}

=(y_5-y_{out})*f(Z_5)*(1-f(Z_5))*W_{53}*f(Z_3)*(1-f(Z_3))*x_1</script><p>上面结果都是已知值，同理可得其他几个式子。</p>
<p>则最终的结果为：</p>
<script type="math/tex; mode=display">\begin{cases}
  W_{31}=W_{31}-\dfrac{\partial C}{\partial W_{31}}=0.09661944 \\
  W_{32}=W_{32}-\dfrac{\partial C}{\partial W_{32}}=0.78985831 \\
  W_{41}=W_{41}-\dfrac{\partial C}{\partial W_{41}}=0.39661944 \\
  W_{42}=W_{42}-\dfrac{\partial C}{\partial W_{42}}=0.58985831 
  \end{cases}</script><p>  再按照这个权重参数进行一遍正向传播得出的Error为0.0165，这个值比原来的0.01805要小，则继续迭代，不断修正权值，使得代价函数越来越小，预测值也不段逼近0.5，直到最后得到无限逼近真实值对应的权值。</p>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><p>从以上反向传播的推导过程可以看出，计算代价函数对最后一层（输出层）的偏导，就为计算倒数第二层的偏导提供了条件，有了倒数第二层的偏导，又可以求出倒数第三层…一直重复这些步骤，就可以将所有层的偏导都计算出来。到这里，想必你就更加清楚什么叫反向传播了吧，其实就是从输出层一层层求偏导。</p>
<p>其实，对每层进行求偏导，按照多元函数的链式法则操作，如果一上来就求输入层的偏导数，那么就要先求出第2层的偏导数，而第2层的偏导数，要用第3层的偏导数去表达…最后发现最关键的是要求出最后一层（输出层）的偏导数，所以尼，只好从输出层开始求，一步步向输入层方向求偏导。</p>
<p>最后多说一句哈，大家可能感觉到很多知识和概念都是如此莫名，也就是说科学知识的体系和人类的认知规律经常不一致。比如说在物理学上，时间的定义先于速度，而我们却是先体会到物体运动的快慢，后面才产生时间的概念，皮亚杰的发生认识论就是讲这个问题的，大家可以去看看这位认知方面的祖师爷理论，你应该会更加体会到，神经网络是多么接近人类的认知模式。</p>
<hr>
<h4 id="more-information"><a href="#more-information" class="headerlink" title="more information"></a>more information</h4><p><a href="https://tigerneil.gitbooks.io/neural-networks-and-deep-learning-zh/content/chapter2.html" target="_blank" rel="noopener">How the backpropagation algorithm works</a></p>
<p><a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="noopener">Principles of training multi-layer neural network using backpropagation</a></p>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow学习笔记（四）-- 深度神经网络（DNN）模型与前向传播算法]]></title>
      <url>/2017/06/18/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89--%20%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88DNN%EF%BC%89%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>深度神经网络（Deep Neural Networks，简称DNN）是深度学习的基础，而要理解DNN，首先我们要搞清楚DNN模型，以下就是对DNN模型和前向传播算法的理解与总结。</p>
<h4 id="从感知机到神经网络"><a href="#从感知机到神经网络" class="headerlink" title="从感知机到神经网络"></a>从感知机到神经网络</h4><p>感知机模型比较简单，它是一个有若干输入和一个输出的模型，如下图：</p>
<p><img src="/images/figures/2017-06-18-01.png" alt="感知机模型"></p>
<p>输出和输入之间学习到一个线性关系，从而得到中间输出结果：</p>
<script type="math/tex; mode=display">z=\sum_{i=1}^mw_ix_i+b</script><p>紧接着是一个神经元激活函数：</p>
<script type="math/tex; mode=display">sign(z)=\begin{cases}
          -1 & z<0 \\
           1 & z>=0
           \end{cases}</script><p>从而得到我们想要的输出结果。很容易看出这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。</p>
<p>而神经网络则在感知机模型上做了扩展，主要有以下三点：</p>
<p>1）加入了隐藏层，隐藏层可以有多层，增强模型的表达能力，如下图所示，当然增加了这么多隐藏层模型的复杂度也增加了好多。</p>
<p><img src="/images/figures/2017-06-18-02.png" alt="神经网络一"></p>
<p>2）输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。</p>
<p><img src="/images/figures/2017-06-18-03.png" alt="神经网络二"></p>
<p>3）对激活函数做扩展，感知机的激活函数是 $sign(z)$ ，虽然简单但是处理能力有限，因此神经网络中一般使用其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：</p>
<script type="math/tex; mode=display">f(z)=\dfrac{1}{1+e^{-z}}</script><p>或者是之前有说过的tanx，softmax和ReLu等，通过使用不同的激活函数，神经网络的表达能力得到了进一步增强。</p>
<h4 id="DNN的基本结构"><a href="#DNN的基本结构" class="headerlink" title="DNN的基本结构"></a>DNN的基本结构</h4><p>DNN可以理解为有很多隐藏层的神经网络，这个很多其实也没有什么度量标准，多层神经网络和深度神经网络DNN其实也是指的一个东西，当然，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）, 名字实在是多。后面我们讲到的神经网络都默认为DNN。</p>
<p>将DNN内部的神经网络按不同层的位置划分可以分为三类，输入层，隐藏层和输出层，如下图所示:</p>
<p><img src="/images/figures/2017-06-18-04.png" alt="DNN基本结构"></p>
<p><strong>层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。</strong> 虽然DNN看起来很发杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系 $z=\sum w_ix_i+b$ 加上一个激活函数 $\kappa(z)$ 。</p>
<p>由于DNN层数很多，则我们的线性关系系数 $w$ 和偏置 $b$ 的数量也就很多了，具体的参数在DNN中是如何定义的尼。</p>
<p>首先我们来看看线性关系系数 $w$ 的定义。以下图一个三层的DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性系数定义为 $w<em>{24}^3$ 。上标3代表线性系数 $w$ 所在的层数，而下标对应的是输出的第三层索引2和输入的第二层索引4。你也许会问，为什么不是 $w</em>{42}^3$ , 而是 $w<em>{24}^3$ 尼？这主要是为了便于模型用于矩阵表示运算，如果是 $w</em>{42}^3$ 而每次进行矩阵运算是 $w^Tx+b$ ，需要进行转置。将输出的索引放在前面的话，则线性运算不用转置,即直接为 $w^Tx+b$ 。总结下，第 $l-1$ 层的第k个神经元到第 $l$ 层的第j个神经元的线性系数定义为 $w_{jk}^l$ 。注意，输入层是没有 $w$ 参数的。</p>
<p><img src="/images/figures/2017-06-18-05.png" alt="w的定义"></p>
<p>再来看看偏倚 $b$ 的定义。还是以这个三层的DNN为例，第二层的第三个神经元对应的偏倚定义为 $b_3^2$ 。其中，上标2代表所在的层数，下标3代表偏倚所在的神经元的索引。同样的道理，第三个的第一个神经元的偏倚应该表示为 $b_1^3$ 。同样的，输入层是没有偏倚参数 $b$ 的。</p>
<p><img src="/images/figures/2017-06-18-06.png" alt="b的定义"></p>
<h4 id="DNN前向传播算法数学原理"><a href="#DNN前向传播算法数学原理" class="headerlink" title="DNN前向传播算法数学原理"></a>DNN前向传播算法数学原理</h4><p>我们已经知道了DNN各层线性关系系数 $w$ ，偏置 $b$ 的定义，再假设我们选择的激活函数是 $\kappa(z)$ ，隐藏层和输出层的输出值为 $a$ ，则对于下图的三层DNN，利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。</p>
<p><img src="/images/figures/2017-06-18-07.png" alt="数学原理"></p>
<p>我们可以推导出对于第二层的输出：</p>
<script type="math/tex; mode=display">a_1^2=\kappa(z_1^2)=\kappa(w_{11}^2x_1+w_{12}^2x_2+w_{13}^2x_3+b_1^2)</script><script type="math/tex; mode=display">a_2^2=\kappa(z_2^2)=\kappa(w_{21}^2x_1+w_{22}^2x_2+w_{23}^2x_3+b_2^2)</script><script type="math/tex; mode=display">a_3^2=\kappa(z_3^2)=\kappa(w_{31}^2x_1+w_{32}^2x_2+w_{33}^2x_3+b_3^2)</script><p>由第二层的输出可以求出第三层的输出：</p>
<script type="math/tex; mode=display">a_1^3=\kappa(z_1^3)=\kappa(w_{11}^3a_1^2+w_{12}^3a_2^2+w_{13}^3a_3^2+b_3^3)</script><p>将上面的例子一般化，假设第 $l-1$ 层有m个神经元，则对于第 $l$ 层的第j个神经元的输出 $a_j^l$ ，我们有：</p>
<script type="math/tex; mode=display">a_j^l=\kappa(z_j^l)=\kappa(\sum_{k=1}^mw_{jk}^la_k^{l-1}+b_j^l)</script><p>其中如果 $l=2$ ，则对于 $a_k^l$ 即为输入层的 $x_k$ 。</p>
<p>从上面可以看出，使用代数法一个个的表述输出比较复杂，而如果使用矩阵法则比较简洁。假设第 $l-1$ 层共有m个神经元，而第 $l$ 层有n个神经元，则第 $l$ 层的线性系数 $w$ 组成了一个n x m的矩阵 $W^l$ ，其中偏置 $b$ 组成了一个n x 1的向量 $b^l$ ，输出层 $a$ 组成了一个n X 1的向量 $a^l$ ，而第 $l-1$ 层的输出 $a$ 组成了一个m x 1的向量 $a^{l-1}$ ，则用矩阵法表示，第 $l$ 层的输出为：</p>
<script type="math/tex; mode=display">a^l=\kappa(z^l)=\kappa(W^la^{l-1}+b^l)</script><p>这个表示方法简洁漂亮，后面我们的讨论都会基于上面的这个矩阵法表示来。</p>
<h4 id="DNN前向传播算法"><a href="#DNN前向传播算法" class="headerlink" title="DNN前向传播算法"></a>DNN前向传播算法</h4><p>上面已经对前向传播做了数学推导，其实归纳起来就是利用我们的若干个权重系数矩阵 $W$ ，偏置向量 $b$ 和输入值向量x进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直运算到输出层，得到输出结果。</p>
<p>输入：总层数L，所有隐藏层和输入层对应的矩阵 $W$ ，偏置向量 $b$ ，输入值向量 $x$ </p>
<p>输出：输出层的输出 $a^L$ </p>
<p>1) 初始化 $a^1=x$ </p>
<p>2) for  $l=2$  to  $L$ ，计算： </p>
<script type="math/tex; mode=display">a^l=\kappa(z^l)=\kappa(W^la^{l-1}+b^l)</script><p>最后的结果就是输出 $a^L$ </p>
<p>可能有人会有这样的困惑：单独看DNN前向传播算法，似乎没有什么大用处，而且这一大堆的矩阵 $W$ ，偏倚向量 $b$ 对应的参数怎么获得呢？怎么得到最优的矩阵和偏倚向量尼？这个我们在讲DNN的反向传播算法时再说。</p>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow学习笔记（三）-- 深度神经网络（DNN）优化之损失函数]]></title>
      <url>/2017/06/12/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89--%20%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88DNN%EF%BC%89%E4%BC%98%E5%8C%96%E4%B9%8B%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
      <content type="html"><![CDATA[<h4 id="损失函数（loss-function）"><a href="#损失函数（loss-function）" class="headerlink" title="损失函数（loss function）"></a>损失函数（loss function）</h4><p>用于决定训练过程如何来“惩罚”网络的预测结果和真实结果的差异，它通常存在于网络的最后一层。本次主要介绍一些神经网络中常用的损失函数以及如何选择合适的损失函数。</p>
<p>损失函数，顾名思义就是模型对数据拟合程度的反映，拟合的越差，损失函数的值就越大。但同时我们也期望，损失函数在比较大时，对应的梯度也要越大，这样的话更新变量的速度就可以加快。我们应该都接触过的“距离”这一概念也可以被用在损失函数里，其对应的就是最小平方误差（MSE）:</p>
<script type="math/tex; mode=display">minC(Y,G(X))=\left|G(x)-Y\right|^2=\sum_{i=1}^n(G(X_i)-y_i)^2</script><p>其中G是我们的模型，它根据输入矩阵x输出一个预测向量G(x)，这个损失函数的直观意义相当明确：预测值G(x)和真实值Y的距离越大、损失也就越大，反而就越小。它的求导过程也是相当平凡的：</p>
<script type="math/tex; mode=display">\dfrac{\partial C}{\partial w}=2\sum_{i=1}^n(G(x_i)-y_i)\dfrac{\partial G(x_i)}{\partial w}</script><p>其中w是模型G中的一个待训练的参数，由于MSE比较简单、所以我们能够从一般意义上来讨论它，事实上损失函数的选择通常都是结合激活函数的，下面将讨论几种使用较多的组合方式。</p>
<h4 id="均方误差损失函数-Sigmoid激活函数"><a href="#均方误差损失函数-Sigmoid激活函数" class="headerlink" title="均方误差损失函数+Sigmoid激活函数"></a>均方误差损失函数+Sigmoid激活函数</h4><p>Sigmoid激活函数的表达式为：</p>
<script type="math/tex; mode=display">\kappa(z)=\dfrac{1}{1+e^{-z}}</script><p> $\kappa(z)$ 的函数图像如下：</p>
<p><img src="/images/figures/2017-06-12-01.jpg" alt="Sigmoid"></p>
<p>从图上可以看出，对于Sigmoid，当z的取值越来越大后，函数曲线变得越来越平缓，意味着此时的导数 $\kappa’(z)$ 也越来越小，同样当z的取值越来越来小时，也有这个问题，仅仅在z取值为0附近时，导数 $\kappa’(z)$ 的取值较大。</p>
<p>而在均方误差+Sigmoid的反向传播算法中，每一层向前递推都要乘以 $\kappa’(z)$ ，得到梯度变化值，这意味着大多时候我们的得到的梯度变化值都很小，导致我们的W,b更新到极值的速度较慢，也就是我们的算法收敛速度较慢、不能接受。</p>
<h4 id="交叉熵损失函数-Sigmoid激活函数"><a href="#交叉熵损失函数-Sigmoid激活函数" class="headerlink" title="交叉熵损失函数+Sigmoid激活函数"></a>交叉熵损失函数+Sigmoid激活函数</h4><p>Sigmoid函数特性导致了反向传播算法收敛速度较慢，那么如何改进呢？一种常见的选择是用交叉熵损失函数来代替均方误差损失函数。</p>
<p>每个样本的交叉熵损失函数形式：</p>
<script type="math/tex; mode=display">J(W,b,a,y)=-y.lna-(1-y).ln(1-a)</script><p>其中.为向量内积。</p>
<p>那么使用了交叉熵函数就能解决Sigmoid收敛速度慢的问题么，我们看下使用交叉熵时，各层 $\phi^l $ 的梯度情况：<br> $\phi^l=\dfrac{\partial J(W,b,a^l,y)}{\partial z^l}<br>=-y\dfrac{1}{a^l}(a^l)(1-a^l)+(1-y)\dfrac{1}{1-a^l}(a^l)(1-a^l)<br>=-y(1-a^l)+(1-y)a^l<br>=a^l-y<br>$ </p>
<p>可见此时 $\phi^l $ 梯度表达式中已经没有 $\kappa’(z)$ ，而均方误差损失函数的梯度是：</p>
<script type="math/tex; mode=display">\dfrac{\partial J(W,b,x,y)}{\partial z^L}=(a^L-y).\kappa'(z)</script><p>对比两者在L层的梯度表达式，就可以看出，使用交叉熵得到梯度表达式没有了 $\kappa’(z)$ ，梯度为预测值和真实值得差距，因此避免了反向传播收敛速度慢的问题。</p>
<p>通常情况下，如果我们使用了sigmoid激活函数，交叉熵损失函数肯定比均方差损失函数好用。</p>
<h4 id="对数似然损失函数-softmax激活函数"><a href="#对数似然损失函数-softmax激活函数" class="headerlink" title="对数似然损失函数+softmax激活函数"></a>对数似然损失函数+softmax激活函数</h4><p>在前面所说的DNN相关知识中，我们都是假设输出是连续可导的，但如果是分类问题，那么输出应该是一个个的类别，我们该如何利用DNN类解决这个问题尼？</p>
<p>比如假设我们有一个三个类别的分类问题，这样我们的DNN输出层应该有三个神经元，假设第一个神经元对应类别一，第二个对应类别二，第三个对应类别三，这样我们期望的输出应该是(1,0,0)，（0,1,0）和(0,0,1)这三种。即样本真实类别对应的神经元输出应该无限接近或者等于1，而非改样本真实输出对应的神经元的输出应该无限接近或者等于0。或者说，我们希望输出层的神经元对应的输出是若干个概率值，这若干个概率值即我们DNN模型对于输入值对于各类别的输出预测，同时为满足概率模型，这若干个概率值之和应该等于1。</p>
<p>DNN分类模型要求是输出层神经元输出的值在0到1之间，同时所有输出值之和为1。很明显，现有的普通DNN是无法满足这个要求的。但是我们只需要对现有的全连接DNN稍作改良，即可用于解决分类问题。在现有的DNN模型中，我们可以将输出层第i个神经元的激活函数定义为如下形式：</p>
<script type="math/tex; mode=display">a_i^L=\dfrac{e^{z_i^L}}{\sum_{j=1}^{n_L}e^{z_j^L}}</script><p>其中， $n<em>L$ 是输出层第L层的神经元个数，也即我们分类问题的类别数。很容易看出，所有的 $a_i^L$ 都是在（0,1）之间的数字，而 $\sum</em>{j=1}^{n_L}e^{z_j^L}$ 作为归一化因子保证了所有的 $a_i^L$ 之和为1。</p>
<p>可以看出仅仅将输出层的激活函数从Sigmoid之类的函数转变为上面的激活函数即可。它就是我们的softmax激活函数，在分类问题中有广泛的应用。</p>
<p>下面通过一个例子介绍softmax激活函数在前向传播算法时的使用。假设输出层有三个神经元，而未激活的输出为3,1和-3，求出各自的指数表达式20,2.7和0.05，我们的归一化因子是22.75，这样我们就求出了三个类别的概率输出分布为0.88,0.12和0。</p>
<p><img src="/images/figures/2017-06-12-02.jpg" alt="Softmax layer"></p>
<p>从上图的计算过程可以看出，将softmax用于前向传播算法是很简单的，但在反向传播算法中还简单么，也就是梯度好计算么，答案是肯定的。</p>
<p>对于用于分类的softmax激活函数，对应的损失函数一般都是用对数似然函数：</p>
<script type="math/tex; mode=display">J(W,b,a^L,y)=-\sum_ky_klna_k^L</script><p>其中 $y_k$ 的取值为0或者1,如果某一训练样本的输出为第i类，则 $y_i=1$ ，其余都为0。由于每个样本只属于一个类别，所以这个对数似然函数可以简化为：</p>
<script type="math/tex; mode=display">J(W,b,a^L,y)=-lna_i^L</script><p>其中i即为训练样本真实的类别序号。</p>
<p>可见损失函数只和真实类别对应的输出有关，这样假设真实类别是第i类，则其他不属于第i类序号对应的神经元的梯度导数直接为0。对于真实类别第i类，它的 $W_i^L$ 对应的梯度计算为：</p>
<script type="math/tex; mode=display">\dfrac{\partial J(W,b,a^L,y)}{\partial W_i^L}=\dfrac{\partial J(W,b,a^L,y)}{\partial a_i^L}\dfrac{\partial a_i^L}{\partial z_i^L}\dfrac{\partial z_i^L}{\partial w_i^L}

=(a_i^L-1)a_i^{L-1}</script><p>同样的可以得到 $b_i^L$ 的梯度表达式为：</p>
<script type="math/tex; mode=display">\dfrac{\partial J(W,b,a^L,y)}{\partial b_i^L}=a_i^L-1</script><p>可见，梯度计算也很简洁，也没有第一节说的训练速度慢的问题。举个例子，假如我们对于第2类的训练样本，通过前向算法计算的未激活输出为（1,5,3），则我们得到softmax激活后的概率输出为：(0.015,0.866,0.117)。由于我们的类别是第二类，则反向传播的梯度应该为：(0.015,0.866-1,0.117)。是不是很简单尼？</p>
<p>当softmax输出层的反向传播计算完以后，后面的普通DNN层的反向传播计算和之前讲的普通DNN没有区别。</p>
<h4 id="梯度爆炸梯度消失和ReLu激活函数"><a href="#梯度爆炸梯度消失和ReLu激活函数" class="headerlink" title="梯度爆炸梯度消失和ReLu激活函数"></a>梯度爆炸梯度消失和ReLu激活函数</h4><p>学习DNN，大家一定听说过梯度爆炸和梯度消失两个词。尤其是梯度消失，是限制DNN与深度学习的一个关键障碍，目前也还没有完全攻克。</p>
<p>什么是梯度爆炸和梯度消失呢？从理论上说都可以写一篇论文出来。不过简单理解，就是在反向传播的算法过程中，由于我们使用的是矩阵求导的链式法则，有一大串连乘，如果连乘的数字在每层都是小于1的，则梯度越往前乘越小，导致梯度几乎消失，进而导致前面的隐藏层的W,b参数随着迭代的进行，几乎没有大的改变，更谈不上收敛了，这个问题目前仍没有完美的解决办法；而如果连乘的数字在每层都是大于1的，则梯度越往前乘越大，导致梯度爆炸，对于这种情况，我们一般可以调整DNN模型中的初始化参数得以解决。</p>
<p>对于无法完美解决的梯度消失问题，目前也有很多研究，一个可能部分解决梯度消失问题的办法是使用ReLu(Rectified Linear Unit)激活函数，ReLu在卷积神经网络中得到了广泛应用，似乎梯度消失不再是问题，其表达式就是我们之前提到的：</p>
<script type="math/tex; mode=display">\phi(z)=max(0,z)</script><p>也就是说大于等于0则不变，小于0则激活后为0。就这么一撮就可以解决梯度消失？至少部分是的。具体原因现在其实也没有从理论上得以证明。</p>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>1）如果使用sigmoid激活函数，则交叉熵损失函数一般肯定比均方差损失函数好</p>
<p>2）如果是DNN用于分类，则一般在输出层使用softmax激活函数和对数似然损失函数</p>
<p>3）ReLU激活函数对梯度消失问题有一定程度的解决，尤其是在CNN模型中</p>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow学习笔记（二）-- 深度神经网络（DNN）优化之激活函数]]></title>
      <url>/2017/06/04/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89--%20%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88DNN%EF%BC%89%E4%BC%98%E5%8C%96%E4%B9%8B%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
      <content type="html"><![CDATA[<h4 id="激活函数（Activation-function）"><a href="#激活函数（Activation-function）" class="headerlink" title="激活函数（Activation function）"></a>激活函数（Activation function）</h4><p>维基百科的解释是，一个节点的激活函数定义了该节点在给定的输入或输入的集合下的输出。不要误解是指这个函数去激活什么，而是指如何把“激活的神经元的特征”通过函数把特征保留并映射出来，这是神经网络能解决非线性问题的关键。</p>
<p>因为神经网络的数学基础是处处可微的，所以选取的激活函数也要保证数据的输入和输出也是可微的，常见的激活函数有：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">类型</th>
<th style="text-align:center">取值范围</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">tanh</td>
<td style="text-align:center">[-1, 1]</td>
<td style="text-align:center">双切正切函数</td>
</tr>
<tr>
<td style="text-align:center">sigmoid</td>
<td style="text-align:center">[0, 1]</td>
<td style="text-align:center">采用S形函数</td>
</tr>
<tr>
<td style="text-align:center">ReLu</td>
<td style="text-align:center">大于0的留下，否则一律删除</td>
<td style="text-align:center">简单而粗暴</td>
</tr>
</tbody>
</table>
</div>
<p>神经网络中，运算特征是在不断进行循环计算，所以在每代的循环过程中，每个神经元的值也是在不断变化的。这就导致tanh特征相差明显时结果会更理想，在循环过程中会不断扩大特征效果显示出来，但有时候特征相差比较复杂或是相差不大时，就需要更加细粒度的分类去判断，这时sigmoid的效果就会好很多。而ReLu就是取max(0, x)，因为神经网络不断反复计算，实际上就变成了它在不断试探如何用一个大多数为0的矩阵刻画数据特征，因为稀疏特性（数据有冗余，但近似程度的最大保留数据特征可以用大多数元素为0的稀疏举证来实现）的存在，反而这种方法运算快效果又好，所以目前大多用ReLu代替sigmoid函数。</p>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>有这样一个需求，将下面的红蓝点进行正确的分类，其实就是二分类问题。</p>
<p><img src="/images/figures/2017-06-04-01.jpg" alt="二分图"></p>
<p>这时可以通过单层感知机算法找到一条合适的线性方程很容易的将它们划分开：</p>
<p><img src="/images/figures/2017-06-04-02.png" alt="单层感知机"></p>
<p>那如果给出的是这样的样本点呢：</p>
<p><img src="/images/figures/2017-06-04-03.png" alt="非线性可分"></p>
<p>可以看到该样本点不是线性可分的，一条直线怎么动都不可能完全正确的将三角形和圆形分开，有同学会说用多个感知器来进行组合，以求获得更大的分类问题，上图我们看下是否可行：</p>
<p><img src="/images/figures/2017-06-04-04.png" alt="多层感知机"></p>
<p>好的，我们已经得到多层感知机分类器了，那么分类能力是否就能将样本点正确分开呢，还是分析一下，我们得到的:</p>
<script type="math/tex; mode=display">y=w_{2-1}(w_{1-11}x_1+w_{1-21}x_2+b_{1-1})+w_{2-2}(w_{1-12}x_1+w_{1-22}x_2+b_{1-2})\\\\+w_{2-3}(w_{1-13}x_1+w_{1-23}x_2+b_{1-3})</script><p>对这个公式合并同类项后得到：</p>
<script type="math/tex; mode=display">y=x_1(w_{2-1}w_{1-11}+w_{2-2}w_{1-12}+w_{2-3}w_{1-13})+x_2(w_{2-1}w_{1-21}+w_{2-2}w_{1-22}\\\\+w_{2-3}w_{1-23})+w_{2-1}b_{1-1}+w_{2-2}b_{1-2}+w_{2-3}b_{1-3}</script><p>大家可以看出，不管它怎么组合，最终都是线性方程的组合，得到的分类器本质上还是一个线性方程，也就仍不能解决非线性问题。</p>
<p>就像下图，直线无论在平面上如何旋转，都不能将样本点分开</p>
<p><img src="/images/figures/2017-06-04-05.png" alt="直线旋转图"></p>
<p>既然是非线性问题，总有线性方程不能正确分类的地方，上面的线性方程组合过程其实类似在做如下三条直线的组合，如图：</p>
<p><img src="/images/figures/2017-06-04-06.png" alt="三条直线组合"></p>
<p>但如果在每一层叠加完后，加入一个激活函数，如图所示：</p>
<p><img src="/images/figures/2017-06-04-07.png" alt="激活函数"></p>
<p>通过这个激活函数映射之后，输出很明显就是一个非线性函数，同理扩展到多个神经元组合，表达能力就会更强。</p>
<p><img src="/images/figures/2017-06-04-08.png" alt="多层激活函数"></p>
<p>和上面线性组合相对应的非线性组合如下：</p>
<p><img src="/images/figures/2017-06-04-09.png" alt="非线性组合"></p>
<p>最后再通过最优化损失函数，不断训练优化，我们可以学习到能够正确分类的曲线，至于具体是什么样，谁也不知道。。maybe是下面的这个</p>
<p><img src="/images/figures/2017-06-04-10.png" alt="非线性曲线"></p>
<p>end, 基于以上我们也就解释了一个观点—激活函数是用来加入非线性因素的，解决线性模型不能解决的问题。</p>
<hr>
<h4 id="more-information"><a href="#more-information" class="headerlink" title="more information"></a>more information</h4><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.740.9413&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Performance Analysis of Various Activation Functions in<br>Generalized MLP Architectures of Neural Networks</a></p>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow学习笔记（一）-- 编程模型]]></title>
      <url>/2017/05/15/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89--%20%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p>TensorFlow中涉及到三种模型，计算模型、数据模型和运行模型，通过这三个角度对TensorFlow开始学习，可以对其工作原理有一个整体的理解。</p>
<h3 id="TensorFlow计算模型——计算图"><a href="#TensorFlow计算模型——计算图" class="headerlink" title="TensorFlow计算模型——计算图"></a>TensorFlow计算模型——计算图</h3><p>计算图是TensorFlow中最基本的一个概念，TensorFlow的所有计算都会被转化为计算图上的节点</p>
<h4 id="计算图的概念"><a href="#计算图的概念" class="headerlink" title="计算图的概念"></a>计算图的概念</h4><p>其实TensorFlow的名字已经说明了最重要的两个概念——tensor和flow，tensor就是张量，可以被简单的理解为多维数组，也就是它的数据结构（后续介绍），而flow则体现了它的计算模型，flow意为“流”，很直观的表达了张量之间通过计算相互转化的过程。TensorFlow是一个通过计算图的方式来表述计算的编程系统，它的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。</p>
<h4 id="计算图的使用"><a href="#计算图的使用" class="headerlink" title="计算图的使用"></a>计算图的使用</h4><p>TensorFlow程序一般可分为两个阶段，在第一个阶段需要定义计算图中所有的计算，第二个阶段为执行阶段（后续介绍），以下是计算定义阶段的示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>], name=<span class="string">"a"</span>)</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">3.0</span>], name=<span class="string">"b"</span>)</span><br><span class="line">result = a + b</span><br></pre></td></tr></table></figure></p>
<p>在这个过程中，TensorFlow会自动将定义的计算转化为计算图上的节点。在TensorFlow中，系统会自动维护一个默认的计算图，通过tf.get_default_graph函数可以获取当前默认的计算图，当然TensorFlow也支持通过tf.Graph函数来生成新的计算图，但不同计算图的上的张量和运算都不会共享。</p>
<p>TensorFlow中的计算图不仅可以用来隔离张量和计算，还提供了管理张量和计算的机制，计算图可以通过tf.Graph.device函数来指定运行计算的设备，这也为TensorFlow使用GPU提供了机制，以下程序指定计算跑在GPU上<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.device(<span class="string">'/gpu:0'</span>)</span><br><span class="line">    result = a + b</span><br></pre></td></tr></table></figure></p>
<h3 id="TensorFlow数据模型——张量"><a href="#TensorFlow数据模型——张量" class="headerlink" title="TensorFlow数据模型——张量"></a>TensorFlow数据模型——张量</h3><h4 id="张量的概念"><a href="#张量的概念" class="headerlink" title="张量的概念"></a>张量的概念</h4><p>在TensorFlow中，所有的数据都通过张量的形式来表示。从功能的角度看，张量可以简单理解为多维数据，其中零阶张量表示标量(scalar)，也就是一个数，第一阶张量为向量(vector)，也就是一个一维数组，第n阶张量可以理解为一个n维数组。但张量在TensorFlow中的实现并不是直接采用数组的形式，它只是对其运算结果的引用，在张量中并没有真正保存数字，它保存的是如何得到这些结果的计算过程，这里还是以向量加法为例，以下得到的是对结果的一个引用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#tf.constant是一个计算，这个计算的结果为一个张量，保存在变量a中</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>], name=<span class="string">"a"</span>)</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">3.0</span>], name=<span class="string">"b"</span>)</span><br><span class="line">result = tf.add(a, b, name = <span class="string">"add"</span>)</span><br><span class="line"><span class="keyword">print</span> result</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Tensor(<span class="string">"add:0"</span>, shape=(<span class="number">2</span>,), dtype=float32)</span><br></pre></td></tr></table></figure></p>
<p>从上面代码可以看出，运行结果是一个张量的结构，主要保存了三个属性：名字(name)、维度(shape)和类型(type)。</p>
<p>张量的第一个属性名字不仅是一个张量的唯一标识，同样也给出了这个张量是如何计算出来的。前面说计算图上的每一个节点代表了一个计算，计算的结果就保存在张量中，所以张量和计算图上所代表的计算结果是对应的，这样张量的命名就可以通过“node:src_output”的形式给出，其中node是节点的名称，src_output表示当前张量来自节点的第几个输出。如上面“add:0”就表示result这个张量是计算节点“add”输出的第一个结果。</p>
<p>张量的维度(shape)，这个属性描述了一个张量的维度信息。还有类型(type)属性，每一个张量会有唯一的类型，TensorFlow会对参与运算的所有张量进行类型的检查，当发现类型不匹配时会报错，如把以上代码改为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>], name=<span class="string">"a"</span>)</span><br><span class="line"></span><br><span class="line">就会报错：</span><br><span class="line">valueError: Tensor conversion requested dtype int32 <span class="keyword">for</span> Tensor <span class="keyword">with</span> dtype float32: <span class="string">'Tensor("b:0", shape=(2,), dtype=float32)'</span></span><br></pre></td></tr></table></figure></p>
<p>TensorFlow支持的数据类型包括，实数(tf.float32、tf.float64)、整数(tf.int8、tf.int16、tf.int32、tf.int64、tf.uint8)、布尔型(tf.bool)和复数(tf.complex64、tf.complex128)。</p>
<h4 id="张量的使用"><a href="#张量的使用" class="headerlink" title="张量的使用"></a>张量的使用</h4><p>和TensorFlow的计算模型相比，其数据模型相对比较简单。张量使用可以总结为两大类，第一类是对中间结果的引用，当计算的复杂度增加时，使用张量来引用中间结果可以大大提高代码的可读性，这样也可以方便的获取中间结果，比如在卷积神经网络中，卷积层或者池化层可能改变张量的维度，通过result.get_shape函数可以获取张量的维度信息，以免去人工计算的麻烦；使用张量的另一类情况是当计算图构造完成后，通过张量(tf.Session().run(result))可以获得计算结果，也就是真是的数字。</p>
<h3 id="TensorFlow运行模型——会话"><a href="#TensorFlow运行模型——会话" class="headerlink" title="TensorFlow运行模型——会话"></a>TensorFlow运行模型——会话</h3><p>TensorFlow使用会话(Session)来执行定义好的运算，会话拥有并管理TensorFlow程序运行时的所有资源。当所有计算完成后需要关闭会话来帮助系统回收资源，否则就可能出现资源泄漏的问题。TensorFlow使用会话的模式一般有两种，第一种模式需要明确调用、关闭会话，如下所示:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个会话</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># 使用这个会话来得到期望的结果</span></span><br><span class="line">sess.run(result)</span><br><span class="line"><span class="comment"># 关闭会话</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></p>
<p>使用这种模式时，当程序因为异常而退出时，关闭会话的函数就不会被执行而导致资源泄漏，为了解决这问题，TensorFlow可以通过python的上下文管理器来使用会话，以下代码展示了这种方式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过python的上下文机制，只要将所有的计算放在“with”内部就可以了</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run()</span><br><span class="line"><span class="comment"># 不需要调用sess.close()来关闭会话</span></span><br><span class="line"><span class="comment"># 当上下文退出时会话关闭和资源释放也就自动完成了</span></span><br></pre></td></tr></table></figure></p>
<p>前面有说过TensorFlow可以会自动生成一个默认的计算图，如果没有特殊指定，运算会自动加入这个计算图中。而会话也有类似的机制，但不会自动生成默认的会话，而是需要手动指定，如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    print(result.eval())</span><br></pre></td></tr></table></figure></p>
<p>以下代码有类似的功能<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">print(result.eval(session=sess)</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></p>
<p>另外在交互式环境下，TensorFlow也提供了一种直接构建默认会话的方式，下面为其用法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 该函数会自动将生成的会话注册为默认的会话</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">print(result.eval())</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></p>
<p>无论使用哪种方法都可以通过ConfigProto Protocol Buffer来配置需要生成的会话<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto(allow_soft_placement=<span class="keyword">True</span>,</span><br><span class="line">                        log_device_placement=<span class="keyword">True</span>)</span><br><span class="line">sess1 = tf.InteractiveSession(config=config) <span class="comment"># or sess2 = tf.Session(config=config)</span></span><br></pre></td></tr></table></figure></p>
<p>参数allow_soft_placement，其值为布尔型，当为True时，在以下任一一个条件成立的时候，GPU上的运算可以放到CPU上进行：</p>
<p>1、运算无法在GPU上执行</p>
<p>2、没有GPU资源(比如运算被指定到第二个GPU上，但机器上只有一个GPU)</p>
<p>3、运算输入包含对CPU计算结果的引用</p>
<p>这个参数的默认值是False，但是为了使得代码的可移植性更强，在有GPU的情况下，这个参数一般会设置为True，保证程序在拥有不同数量GPU的机器上顺利运行。</p>
<p>参数log_device_placement，当值为True时日志中将会记录每个节点被安排在了哪个设备上，以方便调试，不过生产环境中最好置为False以减少产生日志。</p>
]]></content>
      
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hibernate ORM]]></title>
      <url>/2016/11/10/Hibernate%20ORM/</url>
      <content type="html"><![CDATA[<p>一个轻量级的ORM框架，主要不仅负责从Java类到数据库表的映射（还包括从Java数据类型到SQL数据类型的映射），还提供了面向对象的数据查询检索机制，从而极大地缩短的手动处理SQL和JDBC上的开发时间。</p>
<h4 id="ORM基本对应规则"><a href="#ORM基本对应规则" class="headerlink" title="ORM基本对应规则"></a>ORM基本对应规则</h4><ul>
<li>类跟表相对应</li>
<li>类的属性跟表的字段相对应</li>
<li>类的实例与表中具体的一条记录相对应</li>
<li>一个类可以对应多个表，一个表也可以对应多个类</li>
<li>DB中的表可以没有主键，但是类中必须设置主键字段</li>
<li>DB中表与表之间的关系映射成为类之间的关系</li>
<li>object中属性的个数和名称可以和表中定义的字段个数和名称不一样</li>
</ul>
<h4 id="SessionFactory"><a href="#SessionFactory" class="headerlink" title="SessionFactory"></a>SessionFactory</h4><p>负责初始化hibernate，可以被看作是数据源的代理，可以用来创建session对象。SessionFactory是线程安全的，因此可以被多个线程同时访问。SessionFactory在hibernate启动时创建一次（应该使用单例模式来实现）。</p>
<h4 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h4><p>hibernate中的session不是！不是！绝不是！http中所说的session，它是一个轻量级的非线程安全的对象，用来表示应用程序和数据库的一次交互（会话），主要负责被持久化对象和数据库的操作，包含一般的持久化方法CRUD。可以使用SessionFactory创建一个session，当对数据库的所有操作都执行完成后，就可以关闭session，session在访问数据库时才会建立与DB的连接，所以说此对象生存期很短且隐藏了JDBC链接，也是Transaction的工厂。</p>
<h4 id="二级缓存"><a href="#二级缓存" class="headerlink" title="二级缓存"></a>二级缓存</h4><p>一级缓存由session来管理，二级缓存由SessionFactory来管理，在使用时，二级缓存是可有可无的，但一级缓存是必须的。</p>
<p>一级缓存使用的场景：当使用session查询数据时，首先会在session内部查找该对象，若存在，则直接返回，否则就到数据库去查询，并将结果缓存起来以便以后使用。一级缓存的缺点就是当使用session表示一次会话时，它的生命周期较短，而且它是线程不安全的，不能被多个线程所共享，因此对效率的提升不是非常明显。</p>
<p>二级缓存概念：二级缓存用来配置一种全局的缓存，以便实现多个线程与事务共享。在使用了二级缓存机制后，当查询数据时，首先会到内部缓存中查询，如果没有再到二级缓存中查找，最后才去数据库中查找。与一级缓存相比，二级缓存是独立于hibernate软件部分，属于第三方的产品，常见的有Ehcache，OScache等，hibernate提供了CacheProvider接口来充当缓存插件与hibernate之间的适配器。二级缓存除了可以把内存当作存储介质外，还可以选用硬盘等外部存储设备。</p>
<p>二级缓存适用场景：</p>
<ul>
<li>数据量较小，如果数据量太大，缓存太多，会消耗大量内存，造成内存资源短缺，从而降低系统的性能</li>
<li>对数据的修改较少，如果进行大量的修改，就需要频繁的对缓存中的数据和数据库中的数据进行同步，这也会影响系统的性能</li>
<li>不会被大量的应用共享的数据，如果数据被大量线程或事务共享，多线程访问时的同步机制也会影响系统的性能</li>
<li>不是很重要的数据，如果查询的数据很重要（比如财务数据），对数据的正确性要求很高，最好不要使用二级缓存</li>
</ul>
<h4 id="How-提升性能？"><a href="#How-提升性能？" class="headerlink" title="How 提升性能？"></a>How 提升性能？</h4><ul>
<li>延迟加载（当获取一个对象的集合属性值或获取一个对象所关联的另一个对象时）</li>
<li>缓存技术（合理配置缓存的参数，例如配置缓存可以加载数据的个数）</li>
<li>优化查询语句</li>
</ul>
]]></content>
      
        
        <tags>
            
            <tag> ORM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据库连接池]]></title>
      <url>/2016/11/05/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0/</url>
      <content type="html"><![CDATA[<p>对于共享资源，有一个很经典的设计模式就是：资源池（resource pool），这种模式就是要解决资源的频繁分配与释放所造成的问题。</p>
<h4 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h4><p>连接池就是是在系统初始化的时候，将数据库连接作为对象存储在内存中，当用户需要访问数据库时，并非建立一个新的连接，而是从连接池中取出一个已建立的空闲连接对象。使用完毕后，用户也并非将连接关闭，而是将连接放回到连接池中，以便下一个请求访问使用，当应用程序向连接池请求的数量大于最大连接数时，这些请求将被放入到等待队列中。而连接的建立、断开都由连接池自身来管理。</p>
<p>同时还可以通过设置连接池的参数来控制连接池中的初始连接数、连接的上下限数以及每个连接的最大使用次数、最大空闲时间等，也可以通过其自身的管理机制来监视数据库连接的数量、使用情况。</p>
<p>数据库连接池将负责与数据库连接的模块独立出来，方便维护，通过数据库连接池作为中转站，避免了对数据库服务端频繁的连接与断开操作，从而减少了资源占用，获得更快的响应时间。</p>
<h4 id="应用方法"><a href="#应用方法" class="headerlink" title="应用方法"></a>应用方法</h4><p>连接池所处的位置</p>
<p><img src="/images/figures/2016-11-05-01.png" alt="连接池位置"></p>
<p>一般情况下，执行一条sql语句，要经过的流程：</p>
<p><img src="/images/figures/2016-11-05-02.png" alt="sql执行流程"></p>
<p>可以看到需要：</p>
<ul>
<li>TCP建立的连接的三次握手操作</li>
<li>MYSQL认证的三次握手操作</li>
<li>真正的sql执行</li>
<li>mysql的关闭</li>
<li>TCP的四次握手关闭</li>
</ul>
<p>建立连接是一个费时的活动，平均一次花费0.05s~1s的时间，而且系统还要分配内存资源，这样就造成网络IO较多，数据库的负载较高，响应时间较长及QPS（每秒查询率，即规定时间内处理流量多少）较低，而使用连接池后每次访问请求都会复用之前创建的连接。</p>
<p>常用的数据库连接池框架，也都是开发组推荐使用的：</p>
<ul>
<li>C3P0—hibernate</li>
<li>DBCP—Spring</li>
</ul>
]]></content>
      
        
        <tags>
            
            <tag> ORM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Spring事务管理]]></title>
      <url>/2016/10/27/Spring%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86/</url>
      <content type="html"><![CDATA[<h4 id="声明式事务处理"><a href="#声明式事务处理" class="headerlink" title="声明式事务处理"></a>声明式事务处理</h4><p>Spring使用声明式事务处理，默认情况下，如果被注解的数据库操作方法中发生了unchecked异常，所有的数据库操作都将rollback；如果发生的是checked异常，默认情况下数据库操作还是会提交的。</p>
<ul>
<li>unchecked异常：表示错误，程序的逻辑错误。是RuntimeException的子类（java.lang.RuntimeException继承自java.lang.Exception），比如IllegalArgumentException,NullPointException和IllegalStateException。此类异常不需要在代码中显示的捕获做处理。</li>
<li>checked异常：表示无效，不是程序中可以预测的，继承自java.lang.Exception。比如无效的用户输入，文件不存在，网络或数据库链接错误，这些都外在的原因，都不是程序内部可以控制的。必须在代码中显示的处理，比如try-catch块处理，或者给所在的方法加上throws说明，将异常抛到调用栈的上一层。</li>
</ul>
<h4 id="动态代理机制"><a href="#动态代理机制" class="headerlink" title="动态代理机制"></a>动态代理机制</h4><p>Spring中的@Transactional基于动态代理的机制，提供了一种透明的事务管理机制。<strong>Spring事务管理的本质是通过aop为目标类生成动态代理类，并在需要进行事务管理的方法中加入事务管理的横切逻辑代码。</strong></p>
<p>只要给目标类的某个方法加上注解@Transactional，Spring就会为目标类生成对应的代理类，以后调用该类中的所有方法都会先走代理类（即使调用未加事务注解的方法，也会走代理类），即在通过getBean(“ “)获得的业务类时，实际上得到的是一个代理类。（切记：事务逻辑代码在代理类中，@Transactional只是标记此方法在代理类中要加入事务逻辑代码）</p>
<h4 id="Notice"><a href="#Notice" class="headerlink" title="Notice:"></a>Notice:</h4><ul>
<li>在需要事务管理的地方加上@Transactional注解，该注解可以被应用于接口定义和接口方法、类定义和类的public方法上。</li>
<li>@Transactional注解只能应用到public可见度的方法上，如果你在protected、private的方法上使用该注解，它也不会报错，但这个被注解的方法将不会展示已配置的事务配置。</li>
<li>仅仅加上@Transactional注解不能真正开启事务行为，它仅仅是一种元数据，必须在配置文件中使用配置元素，才真正开启事务行为。</li>
<li>通过元素的”proxy-target-class”属性值来控制是基于接口还是基于类的代理被创建，<br>如果属性值被设置为true，那么基于类的代理将起作用（这是需要cglib.jar在classpath中），如果属性值被设置为false或者这个属性被省略，那么标准的JDK基于接口的代理将起作用REQUIRED：如果存在一个事务，则支持当前事务。如果没有事务则开启一个新的事务。</li>
</ul>
<h4 id="Transactional传播机制"><a href="#Transactional传播机制" class="headerlink" title="Transactional传播机制"></a>Transactional传播机制</h4><p>@Transactional(propagation=Propagation.NOT_SUPPORTED)，Propagation支持7种不同的传播机制。传播行为顾名思义，就是当有多个事务的时候，事务之间是如何传播的，传播行为定义事务传播的边界。</p>
<p><img src="/images/figures/2016-10-27-01.png" alt="传播机制"></p>
<p>SUPPORTS： 如果存在一个事务，支持当前事务。如果没有事务，则非事务的执行。但是对于事务同步的事务管理器，PROPAGATION_SUPPORTS与不使用事务有少许不同。</p>
<p>NOT_SUPPORTED：总是非事务地执行，并挂起任何存在的事务。</p>
<p>REQUIRESNEW：总是开启一个新的事务。如果一个事务已经存在，则将这个存在的事务挂起。</p>
<p>MANDATORY：如果已经存在一个事务，支持当前事务。如果没有一个活动的事务，则抛出异常。</p>
<p>NEVER：总是非事务地执行，如果存在一个活动事务，则抛出异常。</p>
<p>NESTED：如果一个活动的事务存在，则运行在一个嵌套的事务中。如果没有活动事务，则按REQUIRED属性执行。</p>
]]></content>
      
        
        <tags>
            
            <tag> Spring </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据库事务隔离级别]]></title>
      <url>/2016/10/20/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/</url>
      <content type="html"><![CDATA[<p>在数据库操作中，为了有效保证并发读取数据的正确性，提出了事务隔离级别的概念</p>
<p>在数据库操作中很可能出现以下不确定情况：</p>
<p><strong>更新丢失（Lost Update）</strong>：两个事务都同时更新一行数据，但第二个事务却中途失败退出，导致对数据的两个修改都失效了。这是因为系统没有执行任何的锁操作，因此并发事务并没有被隔离开来。</p>
<p><strong>脏读（Dirty Reads）</strong>：一个事务开始读取了某行数据，另外一个事务已经更新了此数据但没及时提交。</p>
<p><strong>不可重复读（Non-repeatable Reads）</strong>：一个事务对同一行数据重复读取两次，但是却得到了不同的结果。比如在两次读取的中途，有另外一个事务对该行数据进行了修改并提交。</p>
<p><strong>幻读（Phantom Reads）</strong>：事务在操作过程中进行两次查询，第二次查询的结果包含了第一次查询中未出现的数据（这里不要求两次查询的SQL语句相同），这是因为在两次查询过程中有另外一个事务插入数据。</p>
<p>为了避免出现上面几种情况，定义了4种事务隔离级别，不同的隔离级别对事务的处理不同。</p>
<p><strong>未授权读取，也称为读未提交（Read Uncommitted）</strong>：允许脏读取，但不允许更新丢失。如果一个事务已经开始写数据，则不允许另外一个数据同时进行写操作，但允许其他事务读此行数据。该隔离级别可以通过“排他写锁”实现。</p>
<p><strong>授权读取，也称为读提交（Read Committed）</strong>：允许不可重复读取，但不允许脏读取。读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。</p>
<p><strong>可重复性读取（Repeatable Read）—mysql默认的隔离级别</strong>：禁止不可重复读取和脏读取，但有时可能出现幻影数据。读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。</p>
<p><strong>序列化（Serializable）</strong>：提供严格的事务隔离。它要求事务序列化执行，事务只能一个接着一个的执行，不能并发执行。</p>
<p><img src="http://i2.kiimg.com/595056/dabcb0de993fc078.jpg" alt="事务隔离级别"></p>
<p><strong>死锁</strong>—-是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种相互等待的现象，如果无外力作用，它们都将无法推进。InnoDB在检测到死锁后，会让一个事务释放锁并回退，另一个事务获得锁，以完成事务。</p>
]]></content>
      
        
        <tags>
            
            <tag> database </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[MySql ABC]]></title>
      <url>/2016/10/12/MySql%20ABC/</url>
      <content type="html"><![CDATA[<p>关系型数据库—采用关系模型来组织数据的数据库</p>
<p>关系模型指的就是二维表格模型，而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织。</p>
<h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><h5 id="数值类型"><a href="#数值类型" class="headerlink" title="数值类型"></a>数值类型</h5><p>MySql支持所有标准SQL数值数据类型。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">类型</th>
<th style="text-align:center">大小(字节)</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">TINYINT</td>
<td style="text-align:center">1</td>
<td style="text-align:center">小整数值</td>
</tr>
<tr>
<td style="text-align:center">SMALLINT</td>
<td style="text-align:center">2</td>
<td style="text-align:center">大整数值</td>
</tr>
<tr>
<td style="text-align:center">MEDIUMINT</td>
<td style="text-align:center">3</td>
<td style="text-align:center">大整数值</td>
</tr>
<tr>
<td style="text-align:center">INT或INTEGER</td>
<td style="text-align:center">4</td>
<td style="text-align:center">大整数值</td>
</tr>
<tr>
<td style="text-align:center">BIGINT</td>
<td style="text-align:center">8</td>
<td style="text-align:center">极大整数值</td>
</tr>
<tr>
<td style="text-align:center">FLOAT</td>
<td style="text-align:center">4</td>
<td style="text-align:center">单精度浮点数</td>
</tr>
<tr>
<td style="text-align:center">DOUBLE</td>
<td style="text-align:center">8</td>
<td style="text-align:center">双精度浮点数</td>
</tr>
<tr>
<td style="text-align:center">DECIMAL</td>
<td style="text-align:center">DECIMAL(M, D)，如果M&gt;D为M+2，,否则为D+2</td>
<td style="text-align:center">小数值</td>
</tr>
</tbody>
</table>
</div>
<h5 id="日期和时间类型"><a href="#日期和时间类型" class="headerlink" title="日期和时间类型"></a>日期和时间类型</h5><p>每个时间类型都有一个有效值范围和一个“零”值，当指定不合法的MySql不能表示的值时使用“零”值。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">类型</th>
<th style="text-align:center">大小（字节）</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">DATE</td>
<td style="text-align:center">3</td>
<td style="text-align:center">YYYY-MM-DD</td>
</tr>
<tr>
<td style="text-align:center">TIME</td>
<td style="text-align:center">3</td>
<td style="text-align:center">HH:MM:SS</td>
</tr>
<tr>
<td style="text-align:center">YEAR</td>
<td style="text-align:center">1</td>
<td style="text-align:center">YYYY</td>
</tr>
<tr>
<td style="text-align:center">DATETIME</td>
<td style="text-align:center">8</td>
<td style="text-align:center">YYYY-MM-DD HH:MM:SS</td>
</tr>
<tr>
<td style="text-align:center">TIMESTAMP</td>
<td style="text-align:center">4</td>
<td style="text-align:center">YYYYMMDD HHMMSS</td>
</tr>
</tbody>
</table>
</div>
<h5 id="字符串类型"><a href="#字符串类型" class="headerlink" title="字符串类型"></a>字符串类型</h5><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">类型</th>
<th style="text-align:center">大小（字节）</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">CHAR</td>
<td style="text-align:center">0-255</td>
<td style="text-align:center">定长字符串</td>
</tr>
<tr>
<td style="text-align:center">VARCHAR</td>
<td style="text-align:center">0-65 535</td>
<td style="text-align:center">变长字符串</td>
</tr>
<tr>
<td style="text-align:center">TINYBLOB</td>
<td style="text-align:center">0-255</td>
<td style="text-align:center">不超过255个字符的二进制字符串</td>
</tr>
<tr>
<td style="text-align:center">TINYTEXT</td>
<td style="text-align:center">0-255</td>
<td style="text-align:center">短文本字符串</td>
</tr>
<tr>
<td style="text-align:center">BLOB</td>
<td style="text-align:center">0-65 535</td>
<td style="text-align:center">二进制形式的长文本数据</td>
</tr>
<tr>
<td style="text-align:center">TEXT</td>
<td style="text-align:center">0-65 535</td>
<td style="text-align:center">长文本数据</td>
</tr>
<tr>
<td style="text-align:center">MEDIUMBLOB</td>
<td style="text-align:center">0-16 777 215</td>
<td style="text-align:center">二进制形式的中等长度文本数据</td>
</tr>
<tr>
<td style="text-align:center">MEDIUMTEXT</td>
<td style="text-align:center">0-16 777 215</td>
<td style="text-align:center">中等长度文本数据</td>
</tr>
<tr>
<td style="text-align:center">LONGBLOB</td>
<td style="text-align:center">0-4 294 967 295</td>
<td style="text-align:center">二进制形式的极大文本数据</td>
</tr>
<tr>
<td style="text-align:center">LONGTEXT</td>
<td style="text-align:center">0-4 294 967 295</td>
<td style="text-align:center">极大文本数据</td>
</tr>
</tbody>
</table>
</div>
<h4 id="ACID属性"><a href="#ACID属性" class="headerlink" title="ACID属性"></a>ACID属性</h4><p>原子性</p>
<p>隔离性—-&gt; 在所有的操作没有执行完之前，其他会话不能够看到中间改变的过程</p>
<p>一致性—-&gt; 事务发生前后，数据总额匹配</p>
<p>持久性</p>
<h4 id="InnoDB"><a href="#InnoDB" class="headerlink" title="InnoDB"></a>InnoDB</h4><p>是一个事务型引擎，支持回滚，具有崩溃恢复能力，支持行级锁定、ACID事务。</p>
<p>工作原理—-&gt; 把数据从磁盘加载内存中，被用户进行读写，这样增加了性能。其设计理论就是充分利用内存，减少磁盘IO使用率，每次版本升级，改善最多的就是这些方面。</p>
<h4 id="事务的实现"><a href="#事务的实现" class="headerlink" title="事务的实现"></a>事务的实现</h4><p>mysql在进行事务处理的时候使用的是日志先行的方式，来保证事务可快速和持久运行的，也就是在写数据前，要先写日志—包括事务日志和撤销日志（可以利用撤销日志将数据回滚到修改之前的样子）。</p>
<h4 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h4><p>表的设计—-基本满足第三范式（不存在非关键字段对任一候选关键字段的传递函数依赖）</p>
<p>采用合适的锁机制—-InnoDb引擎属于行级锁（开销大，加锁慢，会出现死锁，但并发度高）</p>
<p>选择合适的事务隔离级别</p>
<p>sql优化与合理利用索引—-生产环境避免使用子查询，可用left join表连接取代；select的字段正好是索引，那么就用到了覆盖索引，通过覆盖索引可以减少I/O，提高性能；加上辅助索引—-sql中or条件，则用不到索引</p>
]]></content>
      
        
        <tags>
            
            <tag> database </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[transient关键字]]></title>
      <url>/2016/10/09/transient%E5%85%B3%E9%94%AE%E5%AD%97/</url>
      <content type="html"><![CDATA[<p>我们都知道一个对象只要实现了Serilizable接口，这个对象就可以被序列化，java的这种序列化模式为开发者提供了很多便利，我们可以不必关系具体序列化的过程，只要这个类实现了Serilizable接口，这个类的所有属性和方法都会自动序列化。</p>
<p>然而在实际开发过程中，我们常常会遇到这样的问题，这个类的有些属性需要序列化，而其他属性不需要被序列化，打个比方，如果一个用户有一些敏感信息（如密码，银行卡号等），为了安全起见，不希望在网络操作（主要涉及到序列化操作，本地序列化缓存也适用）中被传输，这些信息对应的变量就可以加上transient关键字。换句话说，这个字段的生命周期仅存于调用者的内存中而不会写到磁盘里持久化。</p>
<p>总之，java 的transient关键字为我们提供了便利，你只需要实现Serilizable接口，将不需要序列化的属性前添加关键字transient，序列化对象的时候，这个属性就不会序列化到指定的目的地中。</p>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><p>1）一旦变量被transient修饰，变量将不再是对象持久化的一部分，该变量内容在序列化后无法获得访问。</p>
<p>2）transient关键字只能修饰变量，而不能修饰方法和类。注意，本地变量是不能被transient关键字修饰的。变量如果是用户自定义类变量，则该类需要实现Serializable接口。</p>
<p>3）被transient关键字修饰的变量不再能被序列化，一个静态变量不管是否被transient修饰，均不能被序列化。</p>
<p><strong>Notice</strong> 我们知道在Java中，对象的序列化可以通过实现两种接口来实现，若实现的是Serializable接口，则所有的序列化将会自动进行，若实现的是Externalizable接口，则没有任何东西可以自动序列化，需要在writeExternal方法中进行手工指定所要序列化的变量，这与是否被transient修饰无关。</p>
]]></content>
      
        
        <tags>
            
            <tag> Java </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据库的锁机制]]></title>
      <url>/2016/09/28/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E9%94%81%E6%9C%BA%E5%88%B6/</url>
      <content type="html"><![CDATA[<p>数据库管理系统中的并发控制的任务是—确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。在计算机科学，特别是程序设计、操作系统、多处理机和数据库等领域，并发控制（Concurrency control）是确保并及时纠正由并发操作导致的错误的一种机制。</p>
<p>在并发访问情况下，可能会出现脏读、不可重复读和幻读等读现象，为了应对这些问题，主流数据库都提供了锁机制，并引入了事务隔离级别的概念。</p>
<p><strong>锁</strong>—当并发事务访问同一资源时，有可能导致数据不一致，因此需要一种机制来将数据访问顺序化，以保证数据库数据的一致性。锁就是其中的一种机制，在执行多线程时用于强行限制资源访问的同步机制，即用于在并发控制中保证对互斥要求的满足。</p>
<p>MySQL InnoDB默认行级锁。行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁把整张表锁住。</p>
<h4 id="锁的分类"><a href="#锁的分类" class="headerlink" title="锁的分类"></a>锁的分类</h4><p>按操作划分—DML锁、DDL锁</p>
<p>按锁的粒度划分—表级锁、行级锁、页级锁（mysql）</p>
<p>按锁级别划分—共享锁、排它锁</p>
<p>按加锁方式划分—自动锁、显示锁</p>
<p>按使用方式划分—乐观锁、悲观锁</p>
<p>DML锁（data locks, 数据锁）—用于保护数据的完整性，其中包括行级锁、表级锁</p>
<p>DDL锁（dictionary locks, 数据字典锁）—用于保护数据库对象的结构，如表、索引等的结构定义，其中包括排它DDL锁、共享DDL锁、可中断解析锁</p>
<h4 id="乐观锁和悲观锁"><a href="#乐观锁和悲观锁" class="headerlink" title="乐观锁和悲观锁"></a>乐观锁和悲观锁</h4><p>乐观并发控制（乐观锁）和悲观并发控制（悲观锁）就是并发控制主要采用的技术手段。乐观锁和悲观锁是人们定义出来的概念，可以认为是一种思想，其实不仅是关系型数据库系统中有这些概念，像memcache、hibernate等都有类似概念；更不要把他们和数据中提供的提供的锁机制（行锁、表锁、排它锁、共享锁）混为一谈。</p>
<p><strong>悲观锁</strong>—指的是对数据被外界修改持保守态度，在整个数据处理过程中，将数据处于锁定状态。</p>
<p>在关系数据库管理系统中，悲观锁的实现依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能正真保证数据访问的排他性），<strong>数据库中悲观锁的流程</strong>如下：</p>
<ul>
<li><p>在对任意记录进行操作之前，先尝试为该记录加上排它锁（exclusive locking）</p>
</li>
<li><p>如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常，具体响应方式由开发者根据实际需要决定</p>
</li>
<li><p>如果加锁成功，就可以对其做相应操作，直到事务完成之后就会解锁了</p>
</li>
<li><p>这期间如果有其他事务尝试对该记录做修改或是排它锁的操作，都会等待我们解锁或直接抛出异常</p>
</li>
</ul>
<p><strong>优点与不足</strong>—悲观并发控制实际上是“先取锁再访问”的保守策略，为数据处理的安全提供了保证，但是在效率方面加锁机制会让数据库产生额外的开销，还有增加死锁的机会；也会降低并行性，一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以。</p>
<p><strong>乐观锁</strong>—相对悲观锁而言，它假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息。</p>
<p>相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制，一般实现乐观锁的方式就是记录数据版本（为数据增加的一个版本标识（使用版本号或是时间戳），读取数据时将版本标识的值一同读出，数据每更新一次，同时对版本标识进行更新），提交时对比版本信息进行校验。</p>
<p><strong>优点与不足</strong>—乐观并发控制相信事务之间的数据竞争的概率是很小的，，但如果直接简单这么做，还是有可能会遇到不可预期的结果，例如两个事务都读取了数据库的某一行，经过修改以后写回数据库，这时就遇到了问题。</p>
]]></content>
      
        
        <tags>
            
            <tag> database </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Mysql-InnoDB索引]]></title>
      <url>/2016/09/20/Mysql-InnoDB%E7%B4%A2%E5%BC%95/</url>
      <content type="html"><![CDATA[<p><strong>数据库的性能问题</strong></p>
<p>A: 磁盘IO性能非常低，严重的影响数据库系统的性能</p>
<p>B: 磁盘顺序读写比随机读写的性能高很多</p>
<p><strong>数据的基本存储结构</strong></p>
<p>A: 磁盘空间被划分为许多大小相同的块（Black）或者页（Page）</p>
<p>B: 一个表的这些数据块以链表的方式串联在一起</p>
<p>C: 数据是以行（Row）为单位一行一行的存储在磁盘上的块中，如下图</p>
<p>D: 在访问数据时，一次从磁盘中读出或者写入至少一个完整的Block/Page</p>
<p><img src="/images/figures/2016-09-20-01.png" alt="分块图"></p>
<p><strong>数据基本操作的实现</strong></p>
<p>SELECT: 定位数据—&gt; 读出数据所在的块，对数据加工—&gt; 返回数据给用户</p>
<p>UPDATE、DELETE： 定位数据—&gt; 读出数据所在的块，修改数据—&gt; 写回磁盘</p>
<p>INSERT：定位数据要插入的页—＞ 读出要插入的数据页，插入数据—＞ 写回磁盘</p>
<p><strong>定位数据 How</strong></p>
<p>从磁盘中依次读出所有的数据块，一行一行的进行数据匹配，直到找到要匹配的数据，时间复杂度是O(n)，如果所有的数据占用了100个块，尽管只查询一行数据，也需要读出所有100个块的数据，这就需要大量的磁盘IO操作，极大的影响了数据定位的性能。数据定位是所有数据操作必须的，所以其效率也会直接影响数据操作的效率</p>
<p><strong>So 减少磁盘IO How</strong></p>
<p>A: 减少数据占用的磁盘空间—-压缩算法、优化数据存储结构</p>
<p>B: 减少访问数据的总量—-读出或写入的数据中，有一部分是数据操作所必须的，这部分称作有效数据，剩余的则是无效数据，例如，查询姓名是张三的记录，那么这条记录是有效记录，其他则是无效记录，我们要努力减少无效数据的访问。</p>
<p><strong>索引的产生 How</strong></p>
<p>我们发现多数情况下，定位操作不需要匹配整行数据，而是很规律的只匹配某一个或几个列的值，例如id，把这些确定一条数据的列统称为键（key）。<strong>根据减少无效数据访问的原则，我们只将键的值拿过来存放在独立的块中，并且为每一个键值添加一个指针，指向原来的数据块，这就是索引的祖先</strong>。当进行定位操作时，就不再表扫描，而是进行索引扫描，依次读出所有的索引块，进行键值的匹配，当找到匹配的键值后，根据该行的指针直接读取相应的数据块，进行操作。如下图所示：</p>
<p><img src="/images/figures/2016-09-20-02.png" alt="索引产生"></p>
<p><strong>索引的进化</strong></p>
<p>显然这样的效率还不能满足要求，可以通过排序和查找算法来减少IO<br>对索引键进行排序，将索引块的地址（以块为单位）和每一个块的第一行的数据存储到数组，在该数组上实现基于块的折半查找，因为其本身是有序的，可以在此基础上再建立索引块，直到最上层只占用一个块为止。</p>
<p><img src="/images/figures/2016-09-20-03.png" alt="索引进化"></p>
<p>1）这个最上层的Sparse index 称为整个索引树的根root，最底层的数据称为叶子leaf</p>
<p>2）每次进行定位操作时，都从根开始查找，直到叶子节点，才能定位到数据</p>
<p>3）每层索引只需读出一个块</p>
<p>4）索引的IO性能和索引树的高度（索引的层数）成负相关</p>
]]></content>
      
        
        <tags>
            
            <tag> database </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ClassLoader 工作原理与模式]]></title>
      <url>/2016/08/10/ClassLoader%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E4%B8%8E%E6%A8%A1%E5%BC%8F/</url>
      <content type="html"><![CDATA[<h4 id="ClassLoader-what"><a href="#ClassLoader-what" class="headerlink" title="ClassLoader what ?"></a>ClassLoader what ?</h4><p>类加载的动态性体现—-由于Java程序并不是一个原生的可执行文件，而是由许多独立的类文件组成，每一个文件对应一个java类，而这些类文件并非立即全部装入内存，它总是先把保证程序运行的基础类一次性加载到JVM中，其他类等到JVM用到的时候再加载，这样做节省了内存的开销（因为Java最早是为嵌入式系统设计的，内存宝贵），用时再加载就是Java动态性的体现。而ClassLoader就是负责类文件装入到内存的机制。</p>
<h4 id="体系结构"><a href="#体系结构" class="headerlink" title="体系结构"></a>体系结构</h4><p>Java的类装载器实质上也是类，功能是把类载入JVM中，三个类如下：</p>
<p>BootStrap是最顶层的类加载器，用C++语言编写，用来读取Java的核心类库JRE/lib/rt.jar，即系统类</p>
<p>ExtensionClassLoader是用来读取Java的扩展类库JRE/lib/ext/*.jar</p>
<p>AppClassLoader是用来读取classpath指定的所有jar包，即应用类</p>
<p>以上三个装载器存在父子层级关系，从而构成<strong>双亲委托模型</strong></p>
<p>CustomClassLoader是用户自定义编写的，用来读取指定类文件</p>
<h4 id="类的装载方式"><a href="#类的装载方式" class="headerlink" title="类的装载方式"></a>类的装载方式</h4><p><strong>隐式装载</strong>—程序在运行过程中当碰到通过new等方式生成对象时，隐式调用类装载器加载对应的类到JVM中</p>
<p><strong>显式装载</strong>—通过class.forname()等方法，显示加载需要的类</p>
<h4 id="双亲委托模式"><a href="#双亲委托模式" class="headerlink" title="双亲委托模式"></a>双亲委托模式</h4><p>当类装载器有载入类的需求时，会先委托其Parent装载器寻找目标类，如果Parent找不到，那么才由自己依照自己的搜索路径去查找并装载目标类。</p>
<p>使用委托模式也是基于安全来考虑，如果一个恶意的基础类被加载到jvm，会引起严重后果，但使用双亲委托机制，类永远是由根装载器来装载，避免了以上情况的发生。</p>
<h4 id="JVM加载class文件的原理机制—加载的详细过程"><a href="#JVM加载class文件的原理机制—加载的详细过程" class="headerlink" title="JVM加载class文件的原理机制—加载的详细过程"></a>JVM加载class文件的原理机制—加载的详细过程</h4><p>1、装载</p>
<p>2、链接：检查—&gt;准备—&gt;解析—&gt;初始化</p>
<p><img src="/images/figures/2016-08-10-01.png" alt="加载过程"></p>
]]></content>
      
        
        <tags>
            
            <tag> Java </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[HashMap源码解析]]></title>
      <url>/2016/08/03/HashMap%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</url>
      <content type="html"><![CDATA[<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashMap</span>&lt;<span class="title">k</span>, <span class="title">v</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractMap</span>&lt;<span class="title">k</span>, <span class="title">v</span>&gt;</span></span><br><span class="line"><span class="class">            <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">k</span>, <span class="title">v</span>&gt;, <span class="title">Cloneable</span>, <span class="title">Serializable</span></span></span><br></pre></td></tr></table></figure>
<p><img src="http://i4.buimg.com/595056/58a2e62d17df8c1e.png" alt=""></p>
<p>这里的成员方法不外乎是“增删改查”，也反应了我们编写程序时，一定是以”数据“为导向的。</p>
<p>Map虽然不是Collection，但它提供了三种“集合视角”:</p>
<p>set<k> keySet()—-提供key的集合视角</k></p>
<p>Collection<v> values()—-提供value的集合视角</v></p>
<p>Set<map.entry<k, v="">&gt; entrySet()—-提供key-value序对的集合视角，这里用内部类Map.Entry表示序对</map.entry<k,></p>
<h4 id="HashMap的特性"><a href="#HashMap的特性" class="headerlink" title="HashMap的特性"></a>HashMap的特性</h4><ul>
<li><p>线程非安全，允许key和value都为null，而HashTable与之相反</p>
</li>
<li><p>不保证其内部的顺序，而且随着时间的推移，同一元素的位置也可能改变（resize）</p>
</li>
<li><p>put、get操作的时间复杂度都为O(1)</p>
</li>
<li><p>遍历其集合视角的时间复杂度与其容量（槽的个数）和现有元素大小成正比，所以如果要求遍历的性能很高，不要把capacity设置的过高或是把平衡因子设置的过低（当entry数大于capacity * loadFactor时，会进行resize，resize会导致key重新进行rehash，此时元素的位置也就改变了）</p>
</li>
<li><p>线程非安全性，导致多个线程同时对一hashmap做迭代时若有结构上的改变（添加删除entry)，那么会报concurrentModifyException，术语叫fail-fast</p>
</li>
<li><p>Map m = Collections.synchronizedMap(new HashMap(…); 会得到一个线程安全的map</p>
</li>
</ul>
<h4 id="源码剖析"><a href="#源码剖析" class="headerlink" title="源码剖析"></a>源码剖析</h4><p>构造函数—-提供了一个参数为空的构造函数（默认容量16个字符，平衡因子0.75f），和有一个参数且参数类型为Map的构造函数；除此之外，还提供了两个构造函数，用于设置HashMap的容量capacity和平衡因子loadFactor。</p>
<p><strong>工作原理</strong>—-HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode  int值，然后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。</p>
<p><strong>重写hashcode()</strong>—-尽量使用对象本身的属性重写hashcode()，这样如果hashcode不相等，就没有必要再去比较equals，避免增加无谓的计算量。</p>
<p><strong>设计理念</strong>—-HashMap基于散列表实现，采用对象的HashCode可以进行快速查询，增删改查的时间复杂度都是O(1)。</p>
<p><strong>HashMap中的哈希函数设计原理</strong>—-哈希表容量为length，使每个key都能在冲突最小的情况下映射到[0,length)的索引内，hashmap的方法是—让length为2的指数倍，然后用hashCode(key) &amp; (length-1)的方法得到索引。</p>
<p><strong>冲突的解决-链地址法</strong>—-HashMap内部维护了一个Entry数组类型的变量table，用来保存添加进来的Entry对象  transient Entry<k, v="">[] table = (Entry<k, v="">) EMPTY_TABLE;<br>相同索引值的Entry，会以单向链表的形式存在。</k,></k,></p>
<p><strong>HashMap和HashTable的异同</strong>—都实现了Map接口</p>
<ul>
<li><p>同步，线程安全性—HashMap是非synchronized，并可以接受null（HashMap可以接受null的键key和值value，而HashTable则不行），HashTable是synchronized的，这意味着HashTable是线程安全的，多个线程是可以共享一个HashTable，而如果没有正确同步的话，多个线程是不能共享HashMap的。Java5提供了ConcurrentHashMap，它是HashTable的替代，比HashTable的扩展性更好。</p>
</li>
<li><p>速度—由于是HashTable是线程安全的也是synchronized，所以在单线程环境下它比HashMap要慢，如果不需要同步，只需要单一线程，那么使用HashMap性能要好些。sychronized意味着一次仅有一个线程能更改HashTable，任何线程更改HashTable时首先获得同步锁，其它线程要等到同步锁被释放后才能再次获得同步锁更新HashTable<br>HashMap可以通过下面的语句进行同步：Map map = Collections.synchronizeMap(hashMap);</p>
</li>
</ul>
]]></content>
      
        
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[java注解]]></title>
      <url>/2016/07/20/Java%E6%B3%A8%E8%A7%A3/</url>
      <content type="html"><![CDATA[<p>注解这一概念是在java1.5版本提出的</p>
<p>Advantage：本来可能需要很多配置文件和逻辑才能实现的内容，就可以使用一个或者多个注解来替代，这样就使得编程更加简洁</p>
<h4 id="注解的分类"><a href="#注解的分类" class="headerlink" title="注解的分类"></a>注解的分类</h4><p>1）按照运行机制划分：源码注解—-&gt;编译时注解—-&gt;运行时注解</p>
<p><strong>源码注解</strong>：只在源码中才存在，编译成.class文件就不存在了</p>
<p><strong>编译时注解</strong>：在源码和.class文件都存在</p>
<p><strong>运行时注解</strong>：就是在运行阶段还起作用，甚至会影响运行逻辑的注解。像@Autowired自动注入的注解就属于运行时注解，它会在程序运行时把你的成员变量自动的注入进来</p>
<p>2）按照来源划分：来自jdk的注解—-&gt;来自第三方的注解—-&gt;自定义注解</p>
<p>3）元注解就是给注解进行注解</p>
<h4 id="JDK注解分三类"><a href="#JDK注解分三类" class="headerlink" title="JDK注解分三类"></a>JDK注解分三类</h4><p><img src="http://i1.piimg.com/595056/e27d51a89d4aa152.png" alt=""></p>
<h4 id="Java第三方注解"><a href="#Java第三方注解" class="headerlink" title="Java第三方注解"></a>Java第三方注解</h4><p><img src="http://i1.piimg.com/595056/aea070aa5db58d8b.png" alt=""></p>
<h4 id="自定义注解"><a href="#自定义注解" class="headerlink" title="自定义注解"></a>自定义注解</h4><p>1）注解语法要求。前四行就属于元注解<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Target</span>(&#123;ElementType.METHOD,ElementType.TYPE&#125;) <span class="comment">// 注解的作用域，方法和类上</span></span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME) <span class="comment">// 生命周期，运行时存在，可以通过反射读取</span></span><br><span class="line"><span class="meta">@Inherited</span>  <span class="comment">// 标识性的元注解，它允许子注解继承它</span></span><br><span class="line"><span class="meta">@Documented</span>   <span class="comment">// 生成javadoc时会包含注解</span></span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> Description &#123;</span><br><span class="line">    <span class="function">String <span class="title">desc</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function">String <span class="title">author</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">age</span><span class="params">()</span> <span class="keyword">default</span> 18</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>2）使用自定义注解<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Description</span>(desc=<span class="string">"i am Color"</span>,author=<span class="string">"boy"</span>,age=<span class="number">18</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">Color</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"red"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>3）解析注解：通过反射获取类、函数或成员上的运行时注解信息，从而实现动态控制程序运行的逻辑<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ParseAnn</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 使用类加载器加载类</span></span><br><span class="line">            Class c = Class.forName(<span class="string">"com.test.Child"</span>);</span><br><span class="line">            <span class="comment">// 找到类上面的注解</span></span><br><span class="line">            <span class="keyword">boolean</span> isExist = c.isAnnotationPresent(Description.class);</span><br><span class="line">            <span class="comment">// 上面的这个方法是用这个类来判断这个类是否存在Description这样的一个注解</span></span><br><span class="line">            <span class="keyword">if</span> (isExist) &#123;</span><br><span class="line">                <span class="comment">// 拿到注解实例，解析类上面的注解</span></span><br><span class="line">                Description d = (Description) c.getAnnotation(Description.class);</span><br><span class="line">                System.out.println(d.value());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      
        
        <tags>
            
            <tag> Java </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[JVM内存管理--GC]]></title>
      <url>/2016/07/10/JVM%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86--GC/</url>
      <content type="html"><![CDATA[<p>官方解释：Java虚拟机具有一个堆Heap，堆是运行时数据区域，所有类实例和和数组的内存均从此分配。</p>
<p>JVM主要管理两类内存：</p>
<p>堆—-是在Java虚拟机启动时创建，Java代码可及的内存，留给开发人员使用—即存放java对象。</p>
<p>非堆内存—-是JVM留给自己用的，包含方法区（用于存放java类的相关信息，如类名、访问修饰符、常量池、字段描述、方法描述等，在类加载器加载class文件到内存中的时候，JVM会提取其中的类信息，并将这些类信息放到方法区中），JVM内部处理或优化所需的内存、类结构（如运行时常数池，字段和方法数据）以及方法和构造方法的代码—-即存放类加载信息。</p>
<p>在程序运行期间如果想进行垃圾回收，就必须让GC线程与程序当中的线程互相配合，才能在不影响程序运行的前提下，顺利的将垃圾进行回收。</p>
<h3 id="GC策略要解决的问题："><a href="#GC策略要解决的问题：" class="headerlink" title="GC策略要解决的问题："></a>GC策略要解决的问题：</h3><h4 id="1、哪些对象可以被回收"><a href="#1、哪些对象可以被回收" class="headerlink" title="1、哪些对象可以被回收"></a>1、哪些对象可以被回收</h4><p>引用计数算法—-效率高，但对于循环引用的对象无法进行回收</p>
<p>根搜索算法—-jvm普遍采用的算法，原理是设立若干种根对象，当任何一个根对象到某一个对象不可达时，则认为该对象是可以被回收的。</p>
<p>可以当作GC roots的对象有以下几种：</p>
<p>a、虚拟机栈中的引用的对象</p>
<p>b、方法区中的类静态属性引用的对象</p>
<p>c、方法区中的常量引用的对象，主要指的是声明为final的常量值</p>
<p>d、本地方法栈中JNI的引用的对象</p>
<p>a和d都是指的方法的本地变量表</p>
<h4 id="2、什么时候回收这些对象"><a href="#2、什么时候回收这些对象" class="headerlink" title="2、什么时候回收这些对象"></a>2、什么时候回收这些对象</h4><p>概括的说触发主GC的条件：a、当应用程序空闲时，即没有应用程序在运行时，GC会被调用 b、Java堆内存不足时，GC也会被调用。</p>
<p>首先明确一点，GC经常发生的区域是堆区，堆区还可以细分为新生代、老年代，新生代还分为一个Eden区和两个Survivor区。</p>
<p>2.1、对象优先在Eden中分配，当Eden中没有足够空间时，虚拟机将发生一次MinorGC，因为Java大多数对象都是朝生夕灭，所以Minor GC非常频繁，而且速度也很快。</p>
<p>2.2、Full GC，发生在老年代的GC，当老年代没有足够的空间时即发生FullGC，并且一般都会伴随有MinorGC。大对象直接进入老年代，如很长的字符串，虚拟机提供一个参数，令大于这个参数值得对象直接在老年代中分配，避免在Eden区和Survivor区发生大量的内存拷贝。</p>
<p>2.3、发生MinorGC时，虚拟机会检测之前每次晋升到老年代的平均大小是否大于老年代的剩余空间，如果大于，则进行一次FullGC，如果小于，则查看HandlePromotionFailure设置是否允许担保失败，如果允许，那只会进行一次Minor GC，如果不允许，则改为进行一次Full GC。</p>
<h4 id="3、采用什么方式回收"><a href="#3、采用什么方式回收" class="headerlink" title="3、采用什么方式回收"></a>3、采用什么方式回收</h4><p><strong>标记/清除算法</strong>—-原理是当堆中的有效内存空间被耗尽的时候，就会停止整个程序，然后进行两项工作：标记—-其实就是遍历所有的GC roots，然后将所有可达的对象标记为存活对象清除—-遍历堆中的所有对象，将没有标记的对象全部清除掉。</p>
<p><img src="/images/figures/2016-07-10-01.png" alt="标记清除1"><br><img src="/images/figures/2016-07-10-02.png" alt="标记清除2"><br><img src="/images/figures/2016-07-10-03.png" alt="标记清除3"></p>
<p>缺点: 效率比较低（递归与全堆对象遍历），而且在进行GC的时候，需要停止应用程序，导致用户体验差、、这种方式清理出来的空闲内存不是连续的。</p>
<p><strong>复制算法</strong>—-将内存划分为两个区间，在任意时间点上，所有动态分配的对象都只能在其中一个区间（称为活动区间），而另一个区间（称为空闲区间）则是空闲的。当有效内存空间耗尽时，JVM将暂停程序运行，开启复制算法GC线程，会将活动区间内的存活对象全部复制到空闲区间，且严格按照内存地址依次排列，与此同时，GC线程将更新存活对象的内存引用地址指向新的内存地址。空闲区间与活动区间交换的同时，垃圾对象已经被一次性全部回收。不难想象，在下一次GC之后，此时的空闲区间又将变为活动区间，依次循环交替。</p>
<p><img src="/images/figures/2016-07-10-04.png" alt="复制1"><br><img src="/images/figures/2016-07-10-05.png" alt="复制2"></p>
<p>复制算法弥补了标记/清除算法中，内存布局混乱的缺点，但它本身缺点也是很明显的:</p>
<p>1、她浪费了一半的内存，这太要命了 </p>
<p>2、复制这项工作所花费的时间，在对象成活率很高的情况下，将变得不可忽视</p>
<p><strong>标记/整理算法</strong>—-也是分两个阶段：<br>标记：遍历GC roots ，然后将存活的对象标记</p>
<p>整理：移动所有存活的对象，且按照内存地址次序排列，然后将末端内存地址以后的内存全部收回。</p>
<p><img src="/images/figures/2016-07-10-06.png" alt="标记整理1"><br><img src="/images/figures/2016-07-10-07.png" alt="标记整理2"><br><img src="/images/figures/2016-07-10-08.png" alt="标记整理3"></p>
<p>如此一来，当我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可，这比维护一个空闲列表显然少了许多开销。</p>
<p>该算法不仅可以弥补标记/清除算法中内存区域分散的缺点，也消除了复制算法当中内存减半的高额代价，不过它唯一的缺点就是效率不高，不仅要标记所有存活对象，还要整理所有存活对象的引用地址。</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>1、三个算法都是基于根搜索算法去判断一个对象是否应该被回收，而支撑根搜索算法的工作的理论依据，就是语法中变量作用域的内容，因此要防止内存泄漏，最根本办法就是掌握好变量作用域。</p>
<p>2、在GC线程开启时，它们都要暂停应用程序（stop the world）</p>
<p>效率：复制 &gt; 整理 &gt; 清除（此处的效率只是简单的对比时间复杂度，实际情况不一定）</p>
<p>内存整齐度： 复制 = 整理 &gt; 清除</p>
<p>内存利用率： 整理 = 清除 &gt; 复制</p>
<p>标记/清除算法是叫落后的了，但其余两种算法都是在清除算法上优化改进而来的</p>
<p><strong>没有最好的算法，只有更合适的算法</strong></p>
<p><strong>分代搜集算法</strong>—-针对JAVA堆而设计，也就是针对新生代对象和年老代，而存在于方法区的基本上就是不灭对象，不属于分代搜集算法的内容。</p>
<p>新生代对象—-复制算法</p>
<p>年老代对象—-标记/整理算法、标记/清除算法</p>
<p>JVM在进行GC时，大部分时候是普通GC（也就是回收的都是新生代），一般情况下，需要经过好几次普通GC，才会触发一次全局GC（针对年老代）。</p>
]]></content>
      
        
        <tags>
            
            <tag> Java </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[java内存模型]]></title>
      <url>/2016/07/02/java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p>java内存模型规定了如何和何时可以看到其他线程修改的共享变量的值，以及在必须时如何同步的访问共享变量的值。</p>
<h4 id="Java内存模型："><a href="#Java内存模型：" class="headerlink" title="Java内存模型："></a>Java内存模型：</h4><p>1）主要目标是定义程序中变量（指共享变量，存在竞争的变量）的访问规则，就是在虚拟机中将变量存储到内存和从内存中取出这样的底层细节。</p>
<p>2）规定所有的变量都存储在<strong>主内存</strong>中，而每条线程还有自己的<strong>工作内存</strong>，线程的工作内存保存了该线程使用到的变量的主内存中的副本，线程对变量的所有操作都是在工作内存中完成的，而不能直接读写主内存中的变量（volatile变量依然有共享内存的拷贝，但是从工作内存读写数据前，必须先将主内存的数据同步到工作内存中）。</p>
<p>3）不同线程之间也无法访问到对方工作内存中的变量，线程间值得传递都需要通过主内存来完成。</p>
<p><img src="/images/figures/2016-07-02-01.jpg" alt="模型1"><br><img src="/images/figures/2016-07-02-02.jpg" alt="模型2"></p>
<p>Java内存模型往往是指Java虚拟机的运行时内存模型，它规定和指引Java程序在不同的内存架构、cpu和操作系统间有确定性的行为；Java内存模型对一个线程所做的变动能被其他线程可见提供了保证，它们之间是先行发生关系，这种关系确保了：</p>
<p>1）线程内的代码能够按先后顺序执行，这被称为<strong>程序次序规则</strong>（一个线程内保证语义的串行性）</p>
<p>有序性—-在并发时，程序的执行可能就会出现乱序（例读写线程：a=1; flag=true）</p>
<p><strong>乱序 why?</strong> 这个要从cpu指令说起，Java中的代码编译以后，最后也是转换为汇编码的，一条指令的执行是可以分为很多步骤的（取指—译码—执行—写回），当并发时多条指令是一种相对错开的方式并行执行，所以就可能出现乱序执行。。而由于指令间数据相互依赖造成指令等待，从而产生空闲时间，需要改变指令顺序以提高执行效率—-也就是Java内存模型的指令重排，使流水线更加顺畅，当然不能破坏串行程序的语义（可以使用volatile关键字避免优化产生的此类问题）。</p>
<p>指令重排序的意义：能够根据不同处理器的特性（cpu的多级缓存系统，多核处理器）适当的重新排序机器指令，使机器指令更符合cpu的执行特点，最大限度的发挥机器的性能。</p>
<p>2）对于同一个锁，一个解锁操作一定要发生在时间上后发生的另一个锁定操作之前，也叫做<strong>管程锁定规则</strong></p>
<p>3）前一个对volatile的写操作在后一个volatile的读操作之前，也叫<strong>volatile变量规则</strong></p>
<p>4）一个线程内的任何操作必须在这个线程的start（）调用之后，也叫做<strong>线程启动规则</strong></p>
<p>5）一个线程的所有操作都会在线程终止之前，<strong>线程终止规则</strong></p>
<p>6）一个对象的终结操作必须在这个对象构造完成之后，<strong>对象终结规则</strong></p>
<p><strong>Notice</strong>：volatile是一个特殊的修饰符，只有成员变量才能使用它，特性：</p>
<p>1）在Java并发程序缺少同步类的情况下，多线程对被volatile修饰的成员变量的操作对其他线程是透明的，原因是volatile关键字可以保证直接从主存中读取一个变量，如果这个变量被修改后，总是会被写回到主存中去。</p>
<p>2）volatile变量可以禁止指令重排序优化。普通变量仅仅能保证在该方法执行过程中所有依赖的赋值结果都能获得正确结果，而不能保证变量赋值操作顺序和实际执行的顺序一致。</p>
]]></content>
      
        
        <tags>
            
            <tag> Java </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
